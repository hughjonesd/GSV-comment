---
title: "True Lies"
subtitle: "Comment on Garbarino, Slonim and Villeval (2018)"
author: "David Hugh-Jones"
date: "`r Sys.Date()`"
header-includes:
  - \usepackage{setspace}
  - \usepackage{placeins}
  - \onehalfspacing
  - \usepackage{fontspec}
output:
  pdf_document:
    md_extensions: +raw_attribute
    latex_engine: xelatex
  tufte::tufte_handout:
    latex_engine: xelatex
    md_extensions: +raw_attribute
  bookdown::tufte_handout2:
    latex_engine: xelatex
    md_extensions: +raw_attribute
  bookdown::tufte_html2: 
    pandoc_args: ['--self-contained']
    keep_intermediates: true
bibliography: bibliography.bib
link-citations: yes
fontsize: 12pt
mainfont: Times New Roman
mathfont: Times New Roman
---


> **Abstract**: Garbarino, Slonim and Villeval (2018) describe a new method to calculate the probability
> distribution of the proportion of lies told in "coin flip" style experiments. I show that their
> estimates and confidence intervals are flawed. I demonstrate two better ways to estimate the 
> probability distribution of what we really care about -- the proportion of liars -- and
> I provide R software to do this.

```{r setup, include = FALSE}


# how many simulations to run:
nreps <- 1000L

# set to FALSE to recalculate all confidence interval estimates; TRUE for speed:
cache_results <- FALSE

# set to TRUE to (re)create the simulations CSV file, then stop:
rerun_java <- FALSE

params <-   list(
  N      = c(10, 50, 100, 500),
  lambda = seq(0, 1, 0.1),
  P      = c(.2, .5, .8)
)
 
ci_levels <- c(90L, 95L, 99L)


library(checkpoint)
checkpoint("2018-11-03", scanForPackages = cache_results)

library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'), highlight = FALSE)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE)
options(huxtable.knitr_output_format = "latex")

source("bayesian-heads-cts.R")
source("GSV-heads.R")

suppressPackageStartupMessages({
  library(Hmisc)
  library(MASS)
  
  library(dplyr)
  library(purrr)
  library(broom)
  library(readr)
  library(magrittr)
  library(tidyr)
  library(ggplot2)
  
  library(huxtable)
  library(hdrcde)
})

huxtable::set_default_properties(position = "left", top_padding = 12, bottom_padding = 6)
ggplot2::theme_set(theme_light())

set.seed(19751027)

```


Some people are honest, while others are likely to lie whenever it benefits them. We would like to understand
the prevalence of lying, because dishonesty may be economically and socially harmful. Since we
cannot simply ask people if they are liars, one way to estimate the proportion of liars in a group
is to ask them to report the result of a coin flip or other random device, offering them a payment
if they report heads. Liars don't always lie: they only lie when it benefits them. So they always
report heads irrespective of the true coin flip. ^[@Garbarino2018 maintain this assumption and so shall I.] If
there are many more heads than we would expect by chance, we can assume many people are lying. But
how many?

A naïve estimate would be that if e.g. 80 people of 100 report heads, then on average 50 really saw
heads and 60% (30/50) of the remainder are lying. More generally, from *R* reports of a good outcome
in a sample of size *N*, where the _bad_ outcome happens with probability *P*, we can estimate that
the following proportion are lying [@abeler2016preferences]:

\begin{equation}
\label{eqn:naive-estimator}
\frac{R/N - (1-P)}{P}
\end{equation}
The problem with this approach is that the number of heads is not
fixed. If we see 1 out of 3 people reporting heads, this method estimates there are less than zero
liars. But it is still possible that everyone saw heads and 1 person lied.

@Garbarino2018 -- GSV from here on -- point out this problem and introduce an alternative method.
They claim that their method corrects for this problem and can estimate the full distribution of
lying outcomes, and they recommend using it for confidence intervals, hypothesis testing and power
calculations.


```{r generate-data, cache = cache_results}

sim_data <- tidyr::crossing(
  N      = params$N,
  lambda = params$lambda,
  P      = params$P
)

sim_data <- sim_data[rep(1:nrow(sim_data), each = nreps), ]
sim_data$sim <- rep_len(1:nreps, nrow(sim_data))


sim_data %<>% mutate(
        heads = rbinom(nrow(.), N, 1 - P),
        lies = rbinom(nrow(.), N - heads, lambda),
        R    = heads + lies
      )

sim_data %<>% arrange(N, lambda, P)
  

nci_levels <- length(ci_levels)
sim_data <- sim_data[rep(1:nrow(sim_data), each = nci_levels), ]
sim_data$CI <- rep(ci_levels, nrow(sim_data)/nci_levels)


# algorithm is deterministic so we save time by 
# only printing out distinct lines
sim_input <- sim_data %>% 
      select(N, R, P, CI) %>% 
      distinct()

file_root <- "GSV-sims"
sim_output_file <- paste0(file_root, "Output.csv")
if (rerun_java) {
  sim_input_file <- paste0(file_root, ".csv")
  # input has to be in percentages:
  readr::write_csv(sim_input %>% mutate(P = P * 100), path = sim_input_file)

  # the java only lets you select a document from your home directory X-o
  copied_to_home_dir_ok <- file.copy(sim_input_file, normalizePath("~"), overwrite = TRUE)
  stopifnot(copied_to_home_dir_ok)
  stop(sprintf("Now run java -jar LyingCalculator.jar and input '%s'", sim_input_file),
        " from your home directory.\n",
        "Make sure the old output file is deleted first!\n",
        sprintf("The java will create a new file called '%s'.\n", sim_output_file), 
        "Leave it in your home directory, set `rerun_java` to FALSE and reknit this file.")
}

sim_est <- sim_data
```

```{r sanity-checks, include = FALSE, cache = cache_results}
sim_data %<>% mutate(
  expected_R = N * ((1 - P) + lambda * P)
)
summary(lm(R ~ expected_R, sim_data))
sim_data %>% 
      group_by(P, N) %>% 
      do(
        broom::tidy(lm(R ~ expected_R, .))
      ) %>% 
      filter(term == "expected_R")

sim_data$expected_R <- NULL

```

```{r gather-estimates, cache = cache_results}

gsv_estimates <- suppressMessages(readr::read_csv(normalizePath(file.path("~", sim_output_file))))

sim_est %<>% 
      mutate(p_pct = round(P * 100, 0)) %>% 
      left_join(gsv_estimates, by = c("N", "R", "p_pct" = "P", "CI")) %>% 
      mutate(p_pct = NULL)

# work with probs not percentages:
sim_est %<>% 
      mutate(
        LB = LB /100,
        UB = UB / 100,
        EV = EV / 100
      ) %>% 
      rename(
        LB.gsv = LB,
        UB.gsv = UB,
        EV.gsv = EV
      )
```

```{r bayesian-estimates, cache = cache_results}

my_prior <- dunif

bayes_est <- sim_input %>% 
      rowwise() %>% 
      do({
        pstr <- update_prior(
                heads = .$R, 
                N = .$N, 
                P = .$P, 
                prior = my_prior
              )
        cis <- suppressWarnings(dist_hdr(pstr, .$CI/100))
        # calculate quantities as prop dishonest
        tibble(
          EV.ubayes = dist_mean(pstr),
          LB.ubayes = cis[1], 
          UB.ubayes = cis[2]
        )
      })
bayes_est <- cbind(sim_input, bayes_est)
sim_est <- left_join(sim_est, bayes_est, by = c("N", "P", "R", "CI"))

```

```{r simple-estimates, cache = cache_results}

heads_to_liars <- function (prop_heads, p) {
  # excess good outcomes reported over what you'd expect: prop_heads - (1 - p)
  # over the expected proportion of outcomes that were bad: p
  # if there's no excess, assume everyone is honest
  pmax(0, (prop_heads - (1 - p)) / p)
}

simple_est <- sim_input %>%
      mutate(
        EV.simple = heads_to_liars(R/N, P)
      ) 

heads_confints <- apply(sim_input, 1, function (row) {
  ci <- binom.test(row["R"], row["N"], conf.level = row["CI"]/100)$conf.int
  res <- heads_to_liars(ci, row["P"])
  names(res) <- c("LB.simple", "UB.simple")
  res
})

simple_est <- cbind(simple_est, t(heads_confints))
sim_est <- left_join(sim_est, simple_est, by = c("N", "P", "R", "CI"))
```

```{r recalc-estimates, cache = cache_results}

sim_tmp <- sim_input[, c("R", "N", "P", "CI")]
recalc <- apply(sim_tmp, 1, function (x) {
  x <- c(
    prop_liars_ev_gsv(x["R"], x["N"], x["P"]),
    prop_liars_ci_gsv(x["CI"]/100, x["R"], x["N"], x["P"])
  )
  names(x) <- paste0(c("EV", "LB", "UB"), ".recalc")
  x
}) 
sim_tmp <- cbind(sim_tmp, t(recalc))
sim_est <- left_join(sim_est, sim_tmp, by = c("N", "P", "R", "CI"))

# check we aren't too far from their calculations. 
with(sim_est, stopifnot(
        max(abs(EV.recalc - EV.gsv)) < 0.0001,
        mean(abs(LB.recalc - LB.gsv)) < 0.05,
        mean(abs(UB.recalc - UB.gsv)) < 0.05
      ))
```

```{r reshape-results, cache = cache_results}

# reshape to long by method
sim_est %<>%
      tidyr::gather(measure, value, matches("^(LB|EV|UB)\\.")) %>% 
      tidyr::extract(measure, into = c("statistic", "method"), regex = "^(LB|EV|UB)\\.(.*)") %>% 
      tidyr::spread(statistic, value) 


sim_est %<>% mutate(
        within_ci = lambda >= LB & lambda <= UB,
        error     = EV - lambda,
        error_sq  = error^2
      )

# only use recalc in appendix
sim_est_recalc <- sim_est %>% filter(method %in% c("gsv", "recalc"))
sim_est <- sim_est %>% filter(method != "recalc")
```


I ran simulations to check the overall performance of the GSV confidence intervals. Simulations
parameters are shown in Table \ref{tab:params-table}. $\lambda$ is the probability that an individual in the sample lies and report heads
when they observe tails:

\begin{equation}
\label{eqn:lambda}
\lambda = \frac{1}{N} \sum_{i=1}^N \textrm{Prob}(i \textrm{ reports heads}|i\textrm{ saw tails}).
\end{equation}

For each parameter combination, I ran 1000 simulations, drawing random coin flips and reports given
$P$, $\lambda$ and $N$.

For each simulation and confidence level, I computed whether the GSV confidence interval contained
the true value of $\lambda$. The first row of Table \ref{tab:coverage-allmethods} shows the results.


```{r params-table}

huxtable(
        Parameter = c(
          "Sample size (N)", 
          "Probability of lying ($\\lambda$)", 
          "Probability of bad random outcome (P)"
        ),
        Values    = c(
          paste(params$N, collapse = ", "),
          paste(params$lambda, collapse = ", "),
          paste(params$P, collapse = ", ")
        ),
        add_colnames = TRUE
      ) %>% 
      set_caption("Parameter values") %>% 
      set_escape_contents(everywhere, 1, FALSE) %>% 
      set_label("tab:params-table") %>% 
      theme_plain()
```



```{r coverage-allmethods}

coverage <- sim_est %>% 
      group_by(method, CI) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE))

cov_overall <- coverage %>% 
      ungroup() %>% 
      mutate(
        CI = paste0("CI ", CI, "%"),
        Method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian"),
        method = NULL
      ) %>% 
      spread(CI, mean_in_ci) %>% 
      arrange(factor(Method, levels = c("GSV", "Frequentist", "Bayesian"))) 

pct_format <- function (x) sprintf("%.1f%%", x * 100)

ht_overall <- as_huxtable(cov_overall, add_colnames = TRUE) %>% 
      set_caption("Proportion of true results within confidence interval.") %>% 
      set_label("tab:coverage-overall") %>% 
      set_number_format(-1, -1, list(pct_format)) %>% 
      theme_plain()

ht_overall %>% 
      set_caption("Coverage levels for GSV and alternative methods") %>% 
      set_label("tab:coverage-allmethods")

```

By definition, 95% of 95% confidence intervals ought to contain the true value, on average. This is
called "achieving nominal coverage". GSV confidence intervals are
too narrow.^[This problem holds across all simulated probabilities of the low outcome, confidence
levels, and sample sizes. See the appendix.]

To deal with this problem, I test two alternative methods for calculating confidence intervals
on my simulated data. The first ("Frequentist") is the standard method of deriving confidence intervals from a binomial test. The second is a Bayesian method. 

To understand the statistics, start with the probability of getting *R* reports of heads in total, given
$\lambda$. Since individuals report heads either if they see heads with probability $1-P$, or if
they see tails but lie, with probability $\lambda P$, this is just:

\begin{equation}
\label{eqn:binom}
\textrm{Pr}(R|\lambda; N,P) = \textrm{binom}(R, N, (1 - P) + \lambda P)
\end{equation}

That immediately suggests the "Frequentist" method, which is to estimate the parameter of this
distribution, $(1 - P) + \lambda P$, from the proportion of heads reported in the sample, then back
out $\lambda$. This is the conventional method of e.g. @abeler2016preferences. It is justified if
the sample is large, because this will lessen sampling variation in the proportion of actual heads
observed. Similarly, if the sample is large enough, we can generate hypothesis tests for a value of
$\lambda$ -- e.g., zero -- by using the tails of the binomial distribution. And we can back out
confidence bounds for $\lambda$ from confidence bounds for the population proportion of heads
reported in the same way. As GSV point out, in small samples, this method runs the risk that the
sample proportion of high outcomes will be different from its expected value.^[In particular, this
method can estimate confidence bounds for $\lambda$ lower than 0. If so, we can set them to 0.]
We'll see whether this matters.

There are numerous ways to calculate confidence intervals in a test of proportions. See e.g.
@agresti1998approximate. Here, I use the binomial exact test of @clopper1934use, which is 
known to be conservative.

The second method uses Bayes' rule. Start with a prior probability density function over
$\lambda$, $\phi(\lambda)$. The posterior probability is then:

\begin{equation}
\label{eqn:bayes}
\phi(\lambda | R; N,P) = \frac{
  \textrm{Pr}(R|\lambda; N,P) \phi(\lambda)
}{
  \int_0^1 \textrm{Pr}(R|\lambda'; N,P) \phi(\lambda') \; \textrm{d}\lambda'
}
\end{equation}


From this, one can derive confidence intervals and expected values in the usual way. Technically,
they are Bayesian "credible" intervals. I used Highest Posterior Density intervals
[@hyndman1996computing], rather than the central confidence interval. This allows the intervals to
include endpoints of the distribution, which is important when e.g. testing for $\lambda = 0$.

The Bayesian method requires a prior. Here, I used a uniform prior, $\phi(\lambda) = 1$ on $[0, 1]$.

Results in Table
\ref{tab:coverage-allmethods} show that both frequentist and  Bayesian methods mostly achieve the nominal
confidence level, with more than 90/95/99% of intervals containing the true value of *Liars*. The
exception is the frequentist 99% confidence interval, which is too narrow.

Frequentist confidence intervals could be less accurate when $N$ is low, since that leads to more
sampling variation in the number of true heads. Table \ref{tab:coverage-N} checks this by looking
separately at simulations with *N = 10* and *N = 50*. Frequentist 99% confidence intervals indeed
appear slightly too narrow for this range. Bayesian confidence intervals are fine. 


```{r coverage-N}

coverage_N <- sim_est %>% 
      filter(N <= 50) %>% 
      group_by(method, CI, N) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        CI = paste0("CI ", CI, "%"),
        Method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian"),
        method = NULL
      ) %>% 
      select(Method, N, CI, mean_in_ci) %>% 
      spread(CI, mean_in_ci) %>% 
      arrange(factor(Method, levels = c("GSV", "Frequentist", "Bayesian"))) 
  

as_huxtable(coverage_N, add_colnames = TRUE) %>% 
      set_rowspan(c(2, 4, 6), "Method", 2) %>% 
      set_valign(c(2, 4, 6), "Method", "top") %>% 
      set_number_format(-1, contains("CI"), list(pct_format)) %>% 
      set_caption("Confidence interval coverage by sample size") %>% 
      set_label("tab:coverage-N") %>% 
      theme_plain() %>% 
      set_background_color(everywhere, 1, NA) %>% 
      set_top_border(c(4, 6), everywhere, 0.4) %>% 
      set_bottom_border(6, 1, 0.4) %>% 
      set_right_border(everywhere, 1, 0.4)
```


# Understanding the GSV approach

Why does the GSV method produce narrow confidence intervals? We can get a clue by running the GSV method when there are 10 reports of
"heads" out of 10 for a fair coin flip (*R = N = 10, P = 0.5*). The resulting point estimate is that
100% of subjects lied. The upper and lower 99% confidence intervals are also 100%. 

This is calculated as follows. First, given *R* reports of heads, the probability that a
total of $T$ "true" heads were observed is calculated as:


\begin{equation}
\label{eqn:truncated-binom}
\textrm{Prob}(T \textrm{ heads}| R; N, P) = \frac{
  \textrm{binom}(T, N, 1 - P)
}{
  \sum^R_{k=0}\textrm{binom}(k, N, 1 - P)
}
\end{equation}


This is the binomial distribution, truncated at *R* because by assumption, nobody "lies downward"
and reports tails when they really saw heads.

Next, from *T* the number of lies told is calculated as $R - T$; and the proportion of lies told
is:


\begin{equation}
\label{eqn:lies}
\textit{Lies} = \frac{R-T}{N-T} 
\end{equation}


because $N - T$ people saw the low outcome and had the chance to lie. Combining this
with the truncated binomial gives a cumulative distribution function of *Lies*.
This is then used to estimate means and confidence intervals.

Putting these together, for $R = N = 10$, the estimated distribution of *Lies* is calculated as follows:

* With probability $\frac{1}{1024}$, there were really 10 heads. Nobody lied in the
sample.^[But the proportion of people who lied *out of those who saw tails* is undefined, because
noone saw tails. The GSV software seems to resolve this by fixing the proportion of lies to 100%.]

* Otherwise, 1 or more people saw tails, and they all lied. The proportion of liars is 100%.

Hence, the lower and upper confidence intervals are all 100%.

There are two problems with this approach, one statistical, and one conceptual.

```{r heads-10-10-calcs}

heads_10 <- sim_est %>% 
      filter(N == 10, R == 10, P == 0.5) %>% 
      select(lambda, heads) 
h10 <- heads_10$heads
l10 <- heads_10$lambda

inv_prob_h <- round(1/mean(h10 == 10), 0)
inv_prob_h_l20 <- round(1/mean(h10[l10 == 0.2] == 10), 0)

```

First, if many heads are reported, you should learn two things. On the one hand, there are probably
many liars in your sample. On the other hand, probably a lot of coins really landed heads. The
probability distribution in equation (\ref{eqn:truncated-binom}) does not take account of this. 

For example, suppose we are certain that everyone in the sample is a liar who always reports heads.
In this case, observing $R = N = 10$ gives us no information about the true number of heads. The
posterior probability that $T = 10$ is then indeed 1/1024, the same as the prior. Now suppose we
know that nobody in the sample is a liar. Then on observing $R = 10$, we are sure that there were
truly 10 heads: the posterior that $T = 10$ is 1. If exactly 5 out of 10 subjects are liars, then
observing $R = 10$ means that all 5 truth tellers really saw heads. The posterior probability that
$T = 10$ is then $1/32$, the chance that all 5 liars saw heads. And so on.

When we are uncertain about the number of liars, our posterior that $T = 10$ will be some weighted
combination of these beliefs. Unless we are certain everyone in the sample is a liar, the
probability that $T = 10$ will be greater than 1 in 1024. Equation (\ref{eqn:truncated-binom}) is
therefore not correct. In this case, it is equivalent to assuming that everybody in the sample is a liar,
whose report is uninformative about the true number of heads. One then uses the prior distribution
of heads to estimate the proportion of those who actually saw tails and lied.

Indeed, in the simulations with $P =
0.5$ and across all values of $\lambda$, the overall probability that there were 10 true heads,
conditional on $R = N = 10$, was about 1 in `r inv_prob_h`, not 1 in 1024. Fixing $\lambda = 0.2$,
it was about 1 in `r inv_prob_h_l20`.

This problem means that the GSV estimator of *Lies* is biased. In the appendix
I show that the GSV estimator can have substantial bias, and performs worse than the naïve
estimator from equation (\ref{eqn:naive-estimator}), $\frac{R/N-(1-P)}{P}$. Also, the GSV confidence intervals do not always achieve nominal coverage
of *Lies*. When the number of heads reported is either high or low, the percentage of confidence
intervals containing *Lies* may fall below the nominal value.

There is a second, more important problem. The GSV approach attempts to estimate *Lies* in equation
(\ref{eqn:lies}). This is the  proportion of *lies actually told*, among the subsample of people who
saw tails. But we are not usually interested in the proportion of lies actually told. We care about
the probability that a subject in the sample would lie if they saw tails --  $\lambda$ in equation
(\ref{eqn:lambda}). This $\lambda$ can be interpreted in different ways. Maybe on seeing a tail,
each person in the sample lies with probability $\lambda$. Or maybe the sample is drawn from a
population of whom $\lambda$ are (always) liars, and $1 - \lambda$ are truth-tellers. *Lies* has no
interpretation in the population, because the rest of the population has no chance to tell a lie in
the experiment.

*Lies* can be treated as an estimate of $\lambda$. It is unbiased: it estimates $\lambda$ from the
random, and randomly-sized, sample of $N - T$ people who saw tails. But it can be a very noisy
estimate. Again, suppose 10 heads out of 10 are reported, and 9 heads were really observed. *Lies*
is 100%. But it is 100% of just one person.

This means that even the correct confidence intervals for *Lies* would not be correct for $\lambda$.
For example, if 3 out of 3 subjects report heads, the GSV software reports a lower bound of 100% for
any confidence interval. Indeed, since anyone who had the opportunity to lie clearly did so, this is
the correct lower bound (if we arbitrarily define *Lies = 1* when $T = N$). But it
makes no sense as a confidence interval for $\lambda$: we clearly can't rule out that one or two
subjects truly saw heads, and would have reported tails if they had seen tails.

```{r disagg-coverage}

disagg <- sim_est %>% 
      filter(method=="gsv") %>% 
      group_by(
        `R/N:` = R/N, N, P, CI
      ) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE), prop = n()/nrow(.)) %>% 
      mutate(CI = paste0("CI ", CI, "%")) %>% 
      spread(CI, mean_in_ci) %>% 
      mutate(ok  = `CI 95%` > 0.95)

prop_ok <- disagg %>%  
      ungroup() %>% 
      summarize(prop_ok = sum(prop[ok])) %>% 
      pull(prop_ok)
```

Because of this problem, the GSV confidence interval coverage of $\lambda$ is much worse
than its coverage of *Lies*.The issue is especially serious when there are many reports of heads. In
this case there were probably many true heads, so *T* is high and the true sample size $N - T$ is
low, making *Lies* a noisy estimate of $\lambda$. Table \ref{tab:coverage-rn} shows this. It splits
the simulations by the proportion of reported heads, *R/N*. GSV coverage levels fall off sharply as
*R/N* increases. Note that for fair coin flips, *R/N* is usually greater than 0.5, both in the
simulations and in reality.

```{r coverage-rn}
coverage_rn <- sim_est %>% 
      filter(method=="gsv") %>% 
      group_by(
        `R/N` = factor(Hmisc::cut2(R/N, cuts = c(0, 1/4, 1/2, 3/4, 1)))      
      ) %>% 
      mutate(pct_cases = n()/nrow(.)) %>% 
      group_by(`R/N`, CI) %>% 
      summarize(
        mean_in_ci  = mean(within_ci, na.rm = TRUE), 
        `Percentage of simulations` = pct_cases[1]
      ) %>% 
      mutate(CI = paste0("CI ", CI, "%")) %>% 
      spread(CI, mean_in_ci)
    

as_huxtable(coverage_rn, add_colnames = TRUE) %>% 
      set_caption("GSV confidence interval coverage by proportion of heads reported ($R/N$)") %>% 
      set_label("tab:coverage-rn") %>% 
      set_escape_contents(2:3, 1, FALSE) %>% 
      set_number_format(-1, -1, list(pct_format)) %>% 
      theme_plain() %>% 
      set_right_border(-1, 2, 0.4) %>% 
      set_width(0.75)
```


# Point estimation


We can also compare the accuracy of point estimates of $\lambda$ between GSV, Frequentist and
Bayesian methods. Table \ref{tab:bias} shows bias (the estimated value minus the true value of
$\lambda$) for different methods by different *N*. The Bayesian method is always the least biased
until $N = 500$, and the GSV method is the most biased.

Table \ref{tab:mse} shows the mean squared error for methods by different *N*. For low *N*, the best
method is Bayesian and the worst is Frequentist, with GSV in between. When *N* gets large all
methods give about the same estimates and are equally accurate.

The Bayesian method might have an advantage here, since it assumes a uniform prior and the
simulations indeed used a uniform distribution of the proportion of liars *L/N*. In fact, further
analysis reveals that the Bayesian method is best across all specific values of *L/N* up to
80%.^[When *L/N = 1*, all subjects deterministically report heads, and both Frequentist and
GSV point estimates are exactly correct.] So, the Bayesian method is likely to be best unless
one is sure that the true *L/N* is rather high.

```{r point-estimates}

bias_mse <- sim_est %>% 
      filter(CI == 90) %>% 
      mutate(
        Method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian"),
        method = NULL
      ) %>% 
      group_by(Method, N) %>% 
      summarize(
        mean_sq_error = mean(error_sq, na.rm = TRUE),
        bias          = mean(error, na.rm = TRUE)
      )

bias_mse %>% 
      select(-mean_sq_error) %>% 
      spread(N, bias, sep = ": ") %>% 
      ungroup() %>% 
      as_huxtable(add_colnames = TRUE) %>% 
      set_caption("Mean bias by method and $N$") %>% 
      set_label("tab:bias") %>% 
      theme_plain()

bias_mse %>% 
      select(-bias) %>% 
      spread(N, mean_sq_error, sep = ": ") %>% 
      ungroup() %>% 
      as_huxtable(add_colnames = TRUE) %>% 
      set_caption("Mean squared errors by method and $N$") %>% 
      set_label("tab:mse") %>% 
      theme_plain()


```

# Comparing different groups

Bayesian estimates are accurate, but rely on a choice of prior.
A non-informative prior is a reasonable choice. Alternatively one might use information
from previous meta-analyses such as @abeler2016preferences. If the sample size is large enough,
the choice of prior should not matter much.

When comparing the dishonesty rates of different groups, an interesting approach is to use the
"empirical Bayes" method [@casella1985introduction]. This piece of statistical jiu-jitsu involves
estimating a common prior from the pooled data, before updating the prior for each individual group. 

We can also test hypotheses using the Bayesian approach. If two samples are independent, then the 
probability that e.g. the true proportion of liars in sample 1 is smaller than in sample 2 can be
calculated from the posterior distributions for each sample:

\begin{equation}
\label{eqn:hypothesis-testing}
\int^1_0 \int_0^{\lambda_1} \phi(\lambda_1)\phi(\lambda_2) \; \mathrm{d}\lambda_2 \mathrm{d}\lambda_1
\end{equation}


# Applications


```{r benndorf-recalc}
bennd <- update_prior(57, 98, 5/6, dunif)
bennd_bounds <- dist_hdr(bennd, 0.95)
bennd_bounds <- pct_format(bennd_bounds)
bennd_trad <- binom.test(57, 98)$conf.int
bennd_trad <- pct_format(bennd_trad)
```

@benndorf2017experienced use the GSV method to calculate confidence intervals for the proportion of
liars in a lying task with a die roll ($P = 5/6$). From 57 reports of the best outcome, out of 98
subjects, they calculate a lying rate of 49.68%, with a 95% CI of (45.3%, 53.95%). Using the
Bayesian method with a uniform prior, the confidence interval becomes (`r bennd_bounds[1]`, 
`r bennd_bounds[2]`), about twice as big.

@banerjee2018spillover use the GSV method to estimate confidence intervals for proportion of liars
in a die roll task. They estimate the proportion of liars who report a die roll above 3 ($P = 0.5$),
for several treatments. Table \ref{tab:banerjee-recalc} in the appendix shows GSV confidence
intervals, along with recalculated Bayesian confidence intervals (from a uniform prior), and
confidence intervals for the difference between lying to the "Same" and "Other" caste. The Bayesian
confidence intervals are much larger than GSV confidence intervals. Only a couple of significant
results survive. (Note that significance tests in the original paper were done with standard
frequentist techniques, not the GSV method.) More importantly, the N is rather low
to make useful inferences about the differences between groups. For example, for the T2-winners-GC
group in the "aligned payoffs" treatment, differences in lying could be as much as 40% in either
direction.

@hugh2016honesty estimates the dishonesty rates of 15 nations using a coin flip experiment. I use
empirical Bayes to check these results. For my prior over $\lambda$, I fit a beta distribution
using the 15 observations of *2R/N - 1*. I then updated this prior separately for each country to
find new confidence intervals and point estimates of the means.^[Results available on request.]
There is some "shrinkage" towards the pooled mean from the naïve per-country estimates found by
calculating *2R/N - 1* separately for each country. One of the strengths of empirical Bayes, as
@casella1985introduction points out, is that it "anticipates regression to the mean". Using equation
(\ref{eqn:hypothesis-testing}), I calculated the probability of different $\lambda$ values for
each pair of countries in the data. Reassuringly, there were still significant differences between
countries.

# Software

The Bayesian methods described here are implemented in R code, available at https://github.com/hughjonesd/GSV-comment. In this section I give some simple examples of how
to use it. More details are available at the website.

To load the code, download the file "bayesian-heads-cts.R" from github, and source it in the R
command line:

```{r code-example-1, eval = FALSE, echo = TRUE}
source("bayesian-heads-cts.R")
```

Suppose 33 people report heads out of an N of 50, where the probability of the bad outcome is 0.5.
To create a posterior distribution over $\lambda$, we use the `update_prior()` function: 

```{r code-example-2, echo = TRUE}
updated <- update_prior(heads = 33, N = 50, P = 0.5, prior = dunif)
```

Here we've started with a uniform prior, using  R's builtin `dunif()` function.

To calculate the point estimate of lambda, call the `dist_mean()` function on the updated posterior:

```{r code-example-3, echo = TRUE}
dist_mean(updated)
```

To calculate the 95% confidence interval (the highest density region), use `dist_hdr()`:

```{r code-example-4, echo = TRUE}
dist_hdr(updated, 0.95)
```


```{r power-tests}
power_l_25 <- power_calc(N = 100, P = 0.5, lambda = 0.25, nsims = 500)
power_l_10 <- power_calc(N = 100, P = 0.5, lambda = 0.10, nsims = 500)
```

Lastly, we can run power tests by simulating multiple experiments. GSV argue that existing sample
sizes may be too small to reject "no lying" ($\lambda = 0$). With a uniform prior and an $N$ of 100,
the Bayesian method has `r pct_format(power_l_25)` power to detect $\lambda$ of 25% and 
`r pct_format(power_l_10)` power to detect $\lambda$ of 10%. So, this paper confirms that important
point. To run power calculations, use `power_calc()`. Here, we calculate the power to detect
$\lambda = 0.1$ in a sample of 300, where the probability of the bad outcome is 0.5, with an alpha
level of 0.05 and a uniform prior:

```{r code-example-5, echo = TRUE}
power_calc(N = 300, P = 0.5, lambda = 0.1, prior = dunif, alpha = 0.05)
```


# Conclusion

These results suggest some recommendations when designing and analysing a coin-flip
style experiment.
 
1. Use power tests to ensure your N is big enough.

2. If your N is reasonably large, say at least 100, you can safely use standard frequentist 
  confidence intervals and tests.
  
3. If your N is small, consider Bayesian estimates and confidence intervals. To estimate differences 
  between subgroups, consider empirical Bayes with a prior derived from the pooled sample.

Lastly, here is an idea for a simple experimental design to avoid these statistical shenanigans. Ask a
group of *N* experimental subjects, not to flip a coin, but to draw a black or white ball, without
replacement, from an urn containing *N* balls, of which *T* are white. Pay them if they report a
white ball. (Obviously, don't let them observe previous draws.) You will then know -- not estimate --
that the proportion of lies told is $(R - T)/(N - T)$, and this will also be the proportion of
liars, in a sample of exactly $N - T$ subjects. You can compare different samples, or test for no
lying, with a straightforward chi-squared test. The experimenter now knows exactly the
number of liars, and subjects will understand this, which may change the incentives to lie
 (maybe subjects don't like giving the experimenter certain knowledge that somebody lied).
But it simplifies the statistics. This design allows the experimenter to learn the exact proportion 
of lies, and the lying liars who tell them.

\FloatBarrier
\newpage

# Appendix


```{r methods-scale}

scale_colour_methods <- scale_color_brewer(type = "qual", palette = 2, 
      aesthetics = c("colour", "fill"))
```

R code to reproduce this comment is available at https://github.com/hughjonesd/GSV-comment, along with
code to find Bayesian posterior distributions of $\lambda$.

The errors in GSV confidence intervals could be due to a programming error rather than to the
algorithm. I could reproduce GSV's expected value to 4 or 5 significant figures by following their
method, but I could not reproduce their confidence intervals. For example, 
when $R = 3, N = 6, P = 0.5$, GSV's Java program gives the upper bound of the 95% confidence 
interval for the proportion of liars as 49.91%. But the possible proportions of lies when there
are $T = 0, 1, 2, 3$ true heads, are $(R-T)/(N-T) =$
`r paste0(prop_liars(3:0, 3, 6) * 100, "%", collapse = ", ")`.
It may be that some linear interpolation is being done.

Nevertheless, if I use GSV's method as stated, rather than their program, confidence intervals
remain too small, as shown in Table \ref{tab:coverage-recalc}. 

```{r coverage-recalc}

coverage_recalc <- sim_est_recalc %>% 
      filter(method == "recalc") %>% 
      group_by(CI) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        CI = paste0("CI: ", CI, "%"),
      ) %>% 
      spread(CI, mean_in_ci) 

as_huxtable(coverage_recalc, add_colnames = TRUE) %>% 
      set_caption("Proportion of true results within confidence interval, recalculated GSV method") %>% 
      set_label("tab:coverage-recalc") %>% 
      set_number_format(-1, everywhere, list(pct_format)) %>% 
      theme_plain()

```

Figure \ref{fig:coverage-grp-pic} shows the proportions of true values within the confidence interval
for the GSV method, split by *N*, *P*, confidence level and $\lambda$. Dashed lines show the nominal
confidence level. This makes the pattern clear: coverage gets worse as $\lambda$ increases. (At
100%, coverage jumps back up since results become deterministic). Also, coverage does not get
better as *N* increases.


```{r coverage-grp-pic, fig.cap = "GSV confidence interval coverage by confidence level, N, P and $\\lambda$", fig.align="left", fig.width=6, fig.height=4}

pic_method <- "gsv"
# pic_method <- "ubayes"

coverage_grp_gsv <- sim_est %>% 
      filter(method == pic_method) %>% 
      group_by(CI, N, lambda, P) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        P = factor(P),
        ci = CI,
        CI = paste0(CI, "%")
      )

ggplot(coverage_grp_gsv, aes(lambda, mean_in_ci, colour = P)) +
      geom_point() + geom_line() +
      geom_hline(aes(yintercept = ci/100), linetype = 2) +
      facet_grid(N ~ CI, labeller = "label_both") +
      xlab(expression(lambda)) + ylab("Coverage") 

```

Figure \ref{fig:bias-by-lambda-pic} shows the average bias of expected values by different methods.
At N = 500, all methods perform reasonably well. For lower values, there is a clear pattern: 
Bayesian methods are least biased, GSV method is most biased, and the frequentist method is
in between.

```{r bias-by-lambda-pic, fig.cap = "Bias by method and $\\lambda$", fig.align="left", fig.width=5.5, fig.height=4}

bias_lambda <- sim_est %>%       
      filter(CI == 90) %>% 
      group_by(method, N, P) %>% 
      summarize(Bias = mean(error, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian")
      ) %>% 
      rename(Method = method)


ggplot(bias_lambda, aes(Method, Bias, fill = Method)) + 
      geom_col() + scale_x_discrete(breaks = NULL) +
      facet_grid(N ~ P, labeller = "label_both", scales = "free") +
      xlab("") +
      scale_colour_methods

```

Figure \ref{fig:errors-by-true-prop} shows mean squared error by estimation method and $\lambda$,
for *N* of 10 and 50. The Bayesian method is best for all values of *Liars* up to 80%.

```{r errors-by-true-prop, fig.cap = "Errors by method and $\\lambda$, N = 10 and 50", fig.align="left", fig.width=5.5, fig.height=4}

errors_lambda <- sim_est %>%       
      filter(CI == 90, N <= 50) %>% 
      group_by(method, lambda) %>% 
      summarize(mean_sq_error = mean(error_sq, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian")
      ) %>% 
      rename(Method = method)

ggplot(errors_lambda, aes(lambda, mean_sq_error, colour = Method)) + 
      scale_x_continuous(breaks = 0:5/5) + 
      geom_line() + geom_point() +
      scale_colour_methods + 
      xlab(expression(lambda)) + ylab("Mean square error")

```

```{r errors-disagg, include = FALSE, fig.align="left", fig.width=5.5, fig.height=4, fig.cap="Point estimate errors by R/N, N and P"}

errors_disagg <- sim_est %>%       
      filter(CI == 90, N <= 50) %>% 
      group_by(rn = findInterval(R/N, 0:10/10, rightmost.closed = TRUE), N, P, method) %>% 
      summarize(mean_sq_error = mean(error_sq, na.rm = TRUE)) %>% 
      mutate(
        method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian")
      ) %>% 
      rename(`R/N` = rn, Method = method)

g <- ggplot(errors_disagg, aes(`R/N`, mean_sq_error, colour = Method)) + 
      geom_line() + geom_point() +
      facet_wrap(~N + P, scales = "free_y") + 
      scale_colour_methods +
      ylab("Mean square error") 
      
```

 

```{r gsv-lies-coverage, cache = cache_results}

# if we assume that liars are not uniformly distributed, the posterior of number of heads
# is not like the prior
nreps_l <- 2000L

check_coverage_lies <- function(N, CL, lambda, P) {

  L <- rbinom(nreps_l, N, lambda) # how many people will lie if they see tails?
  HL <- rbinom(nreps_l, L, 1 - P)
  HNL <- rbinom(nreps_l, N - L, 1 - P)
  R <- L + HNL
  TH <- HL + HNL
  sim_inputs <- data.frame(L, R, TH, N, CL, P, lambda, row.names = NULL)

  sim_in_uq <- distinct(sim_inputs[, c("CL", "R", "N", "P")])
  sim_results <- apply(sim_in_uq, 1, function (r) {
    with(as.list(r), 
      c(prop_liars_ev_gsv(R, N, P), prop_liars_ci_gsv(CL, R, N, P))
    )
  })
  sims_in_uq <- cbind(sim_in_uq, t(sim_results))
  
  sims <- left_join(sim_inputs, sims_in_uq, by = names(sim_in_uq))
  names(sims) <- c('L', 'R', 'TH', 'N', 'CL', 'P', 'lambda', 'ev', 'cilo', 'cihi')
  
  return(sims)
}

# reuse our parameters
lies_params <- tidyr::crossing(
        N      = params$N,
        lambda = params$lambda,
        P      = params$P,
        CL     = ci_levels/100
      )

lies_sims <- apply(lies_params, 1, function (x) check_coverage_lies(
      x["N"], x["CL"], x["lambda"], x["P"]))
lies_sims <- bind_rows(lies_sims)
lies_sims <- right_join(lies_params, lies_sims, by = names(lies_params))

lies_sims %<>% 
      mutate(
        cl          = CL,
        CL          = paste("CI: ", CL * 100, "%"),
        ci_width    = cihi - cilo,
        lies_told   = (R - TH)/(N - TH),
        lies_in_ci  = cilo <= lies_told & lies_told <= cihi,
        error       = ev - lies_told,
        naive_ev    = pmax(0, (R/N - (1 - P))/P),
        naive_error = naive_ev - lies_told,
        P = factor(P)
      ) %>% 
      group_by(P, N, lambda) %>% 
      mutate(
        # within-group quantiles
        RN = ntile(R/N, 10)
      ) %>% 
      ungroup()


# confints ought to look like real dist, over R, N and P.
qs <- c(0.005, 0.025, 0.05, 0.95, 0.975, 0.995)
lies_sims %<>% group_by(R, N, P) %>%
      do(lies_quantiles = quantile(.$lies_told, qs, na.rm = TRUE)) %>% 
      unnest(lies_quantiles) %>% 
      mutate(quantile = paste0("q", rep_len(qs, nrow(.)))) %>% 
      spread(quantile, lies_quantiles) %>% 
      inner_join(lies_sims, by = c("R", "N", "P")) %>% 
      mutate(
        true_cihi = case_when(
          cl == 0.99 ~ q0.995,
          cl == 0.95 ~ q0.975,
          cl == 0.90 ~ q0.95
        ),
        true_cilo = case_when(
          cl == 0.99 ~ q0.005,
          cl == 0.95 ~ q0.025,
          cl == 0.90 ~ q0.05
        )
      ) %>% 
      select(-starts_with("q0"))
  
```

## GSV as an estimate of *Lies*

GSV argue that their method provides a good estimate of *Lies*, as opposed to $\lambda$. Here
I check whether that is true.

I ran `r nreps_l` simulations for each of the parameter combinations. I calculated *Lies* as
*(R - T)/(N - T)*, and ignored simulations where *N = T*. I estimated confidence intervals and
expected value using the GSV method. For a comparison I also estimated the expected value of 
*Lies* using the "naïve" estimator $\max(0, \frac{R - (1 - P))}{P})$.

Figure \ref{fig:gsv-lies-bias-pic} shows average bias by sample size, *P* and true *Lies*. For low
sample sizes, both methods perform about the same. For higher sample sizes and low values of *Lies*, however, the GSV method is clearly dominated by the naïve method, and shows a lot of upward bias
-- more than 5 percentage points even when *N = 100*. This is especially problematic for testing 
whether anyone lied in the sample.


```{r gsv-lies-bias-pic, fig.cap = "Bias of GSV method for 'Lies'.", fig.align="center", fig.width=5.5, fig.height=4, warning=FALSE}

lies_sims$lt_cut <- cut(lies_sims$lies_told, 0:10/10, labels = FALSE)

my_alph <- 0.8
g <- ggplot(lies_sims, aes(lt_cut, error, color = "GSV", linetype = "GSV")) + 
      stat_summary(fun.y = mean, geom = "point", alpha = my_alph) + 
      stat_summary(group = 1, fun.y = mean, geom = "line", alpha = my_alph) + 
      stat_summary(aes(y = naive_error, color = "Naive"), fun.y = mean, geom = "point", 
            alpha = my_alph) + 
      stat_summary(aes(y = naive_error, linetype = "Naive", color = "Naive"), group=1, fun.y = mean, 
            geom = "line", alpha = my_alph) + 
      facet_grid(N ~ P, labeller = "label_both", scales = "free_y")
g +
      geom_hline(yintercept = 0) +
      scale_x_continuous(breaks = seq(0, 10, 2), labels = seq(0, 1, 0.2)) +
      scale_linetype_discrete(name = "Method", guide = guide_legend(title = "Method")) +
      scale_color_manual(name = "Method", values = c("darkred", "blue")) +
      xlab("Lies") + ylab("Bias") 
```


Figure \ref{fig:gsv-lies-coverage-pic} shows the proportion of confidence intervals that contain the true
value of *Lies*. Coverage is shown by confidence level, *N*, *P* and the decile (within these groups) of
proportion of heads reported (*R/N*). Overall results are quite solid, but when the proportion of
heads reported is low or high, coverage drops below the nominal intervals. This is probably because,
at these extremes, the difference between the true posterior and equation \ref{eqn:truncated-binom} 
becomes large. Interestingly, this problem gets worse as $N$ increases.

 

```{r gsv-lies-coverage-pic, fig.cap = "Coverage of GSV method for 'Lies'.", fig.align="center", fig.width=5.5, fig.height=4}


# you can do this to find where confints are wrong. If the red line is above the diagonal,
# cilo is too high; if the blue line is below it, cihi is too low.
# ggplot(lies_sims) + 
#       geom_smooth(aes(true_cihi, cihi), color = "blue") + 
#       geom_smooth(aes(true_cilo, cilo), color = "red") + 
#       geom_abline(slope = 1, intercept = 0) + 
#       facet_grid(N ~ CL) + theme_minimal() + xlab("True bound") + 
#       labs(title = "Red line: low ci; blue line: high ci")

# lsims_summary <- lies_sims %>% 
#       group_by(lambda, P, N, CI, ci) %>% 
#       summarize(
#         Coverage = mean(lies_in_ci, na.rm = TRUE)
#       )

# ggplot(lsims_summary, aes(lambda, Coverage, colour = P)) + 
#       geom_point() + geom_line() + 
#       geom_hline(aes(yintercept = ci), linetype = 2) +
#       facet_grid(N ~ CI) + xlab(expression(lambda))

lsims_by_R <- lies_sims %>% 
      group_by(P, N, CL, RN, cl) %>%   # having cl keeps it in the data
      summarize(
        Coverage = mean(lies_in_ci, na.rm = TRUE)
      )

ggplot(lsims_by_R, aes(RN, Coverage, colour = P)) + 
      geom_point() + geom_line() + 
      geom_hline(aes(yintercept = cl), linetype = 2) +
      facet_grid(CL ~ N, scales = "free", labeller = "label_both") +
      xlab("R/N (decile)") + 
      scale_x_continuous(breaks = 0:5 * 2)


```

## Banerjee et al. results

Table \ref{tab:banerjee-recalc} recalculates confidence intervals from @banerjee2018spillover using
data in their Tables 6A and 6B.

```{r banerjee-recalc, cache = cache_results}

paste_combn <- function (x, y, flip = FALSE) {
 res <- outer(x, y, FUN = paste0)
 if (! flip) res <- t(res)
 c(res)
}

sigstar <- function (x) {
  symnum(x, corr = FALSE, 
          cutpoints = c(0, .001, .01, .05, 1),
          symbols = c("***", "**", "*", "")
        )
}


treats <- paste_combn(c("T1", "T2", "T2-winners", "T2-losers"), c("-GC-", "-SC-"))
treats <- paste_combn(treats, c("Same", "Other"))
treats <- c("T0-GC-Same", "T0-SC-Same", treats)
treats <- paste_combn(treats, c("-Aligned", "-Unaligned"), flip = TRUE)

banerjee <- data.frame(
  Treatment = treats,
  N       = c(84, 84, rep(c(43, 41, 41, 43, 12,  17, 33, 22), each = 2),
              90, 78, rep(c(42, 42, 40, 44, 12, 15, 32, 28), each = 2)),
  pct_gt3 = c(77.38, 77.38, 83.72, 65.12, 80.49, 73.17, 75.61, 53.66, 88.37, 81.40, 58.33, 50.00, 
              70.59, 52.94, 87.88, 45.45, 68.18, 72.73,
              68.89, 66.67, 57.14, 73.81, 57.14, 66.67, 60.0, 77.5, 56.82, 59.09, 33.33, 41.67,
              40.00, 73.33, 53.12, 93.75, 60.71, 75.00
            )
)

banerjee %<>% mutate(
        N_gt3 = round(pct_gt3/100*N, 0)
      ) %>% 
      extract(Treatment, c("Treatment", "Target", "Payoffs"), regex = "(.*)-([:alnum:]+)-([:alnum:]+)")


banerjee_gsv_cis <- apply(banerjee %>% select(N_gt3, N), 1, function(x) {
  prop_liars_ci_gsv(0.95, x["N_gt3"], x["N"], 0.5)
})

rownames(banerjee_gsv_cis) <- c("gsv_lo", "gsv_hi")
banerjee %<>% cbind(t(round(banerjee_gsv_cis, 2) * 100))


# didn't converge to give finite density at 0!
# pp <- suppressWarnings(MASS::fitdistr(pmax(0.03, 2 * banerjee$N_gt3/banerjee$N - 1), "beta", start = list(
#         shape1 = 1, shape2 = 1
#       )))
# pooled_prior <- function (x) dbeta(x, pp$estimate["shape1"], pp$estimate["shape2"])


banerjee_bayes_cis  <- apply(banerjee %>% select(N_gt3, N), 1, function(x) {
  pstr <- update_prior(x["N_gt3"], x["N"], 0.5, prior = dunif)
  dist_hdr(pstr, 0.95)
})

rownames(banerjee_bayes_cis) <- c("bayes_lo", "bayes_hi")
banerjee %<>% cbind(t(round(banerjee_bayes_cis, 2) * 100))


bj_wide <- banerjee %>% 
      reshape2::melt(id.vars = c("Treatment", "Target", "Payoffs")) %>% 
      reshape2::dcast(Treatment + Payoffs ~ ...)

same_other_p <- apply(bj_wide %>% select(matches("(Other|Same)_N")), 1, function (x) {
  if (is.na(x["Other_N_gt3"])) return(rep(NA, 2))
  pstr_other <- update_prior(x["Other_N_gt3"], x["Other_N"], 0.5, prior = dunif)
  pstr_same <- update_prior(x["Same_N_gt3"], x["Same_N"], 0.5, prior = dunif)
  diff_dist <- difference_dist(pstr_other, pstr_same)
  dist_hdr(diff_dist, 0.95, bounds = c(-1, 1))
})

rownames(same_other_p) <- c("diff_lo", "diff_hi")
same_other_p <- t(same_other_p)
bj_wide %<>% cbind(same_other_p)
bj_wide %<>% select(Treatment, Payoffs, diff_lo, diff_hi)


banerjee %<>% left_join(bj_wide, by = c("Treatment", "Payoffs"))
banerjee %<>% mutate(
        GSV_CIs      = sprintf("%s -- %s", gsv_lo, gsv_hi),
        Bayesian_CIs = sprintf("%s -- %s", bayes_lo, bayes_hi),
        diff_ci      = ifelse(is.na(diff_lo), "", 
                               sprintf("%.0f -- %.0f", diff_lo * 100, diff_hi * 100)),
        diff_ci      = ifelse(diff_lo > 0 | diff_hi < 0, paste(diff_ci, "*"), diff_ci )
      )
```

```{r banerjee-recalc-table}

group_cols <- c("Treatment", "Payoffs", "Diff. 95% CI", "N")
banerjee %>% 
      select(
        Treatment,
        Payoffs,
        Target,
        "Pct > 3"        = pct_gt3, 
        N, 
        "GSV 95% CI"   = `GSV_CIs`, 
        "Bayes 95% CI" = `Bayesian_CIs`,
        "Diff. 95% CI" = `diff_ci`
      ) %>%
      as_huxtable(add_colnames = TRUE) %>% 
      set_rowspan(evens, group_cols, 2) %>% 
      set_rowspan(grepl("T0", banerjee$Treatment), group_cols, 1) %>% 
      set_all_border_colors("grey70") %>% 
      theme_plain() %>% 
      set_top_border(evens, everywhere, 0.4) %>%
      set_bottom_border(evens, group_cols, 0.4) %>%
      set_bottom_border(grepl("T0", banerjee$Treatment), group_cols, 0) %>%
      set_valign(everywhere, group_cols, "middle") %>% 
      set_background_color(NA) %>% 
      set_top_padding(8) %>% 
      set_bottom_padding(4) %>% 
      set_font_size(10) %>% 
      set_caption("Confidence intervals from Banerjee et al. 2018, original and recalculated") %>% 
      set_label("tab:banerjee-recalc")

```

\FloatBarrier

\newpage

# References
