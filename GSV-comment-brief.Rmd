---
title: "Lies and the lying liars who tell them"
subtitle: "Comment on Garbarino, Slonim and Villeval (2018)"
author: "David Hugh-Jones"
date: "`r Sys.Date()`"
header-includes:
  - \usepackage{setspace}
  - \usepackage{placeins}
  - \onehalfspacing
  - \usepackage{fontspec}
output:
  pdf_document:
    md_extensions: +raw_attribute
    latex_engine: xelatex
  tufte::tufte_handout:
    latex_engine: xelatex
    md_extensions: +raw_attribute
  bookdown::tufte_handout2:
    latex_engine: xelatex
    md_extensions: +raw_attribute
  bookdown::tufte_html2: 
    pandoc_args: ['--self-contained']
    keep_intermediates: true
bibliography: bibliography.bib
link-citations: yes
fontsize: 12pt
mainfont: Times New Roman
---


>  Garbarino, Slonim and Villeval (2018) describe a new method to calculate the probability distribution 
>  of the number of liars in "coin flip" style experiments. However, their method  
>  gives misleading confidence intervals. I explain why, and describe two better
>  ways to calculate probability distributions.

```{r setup, include = FALSE}

library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE)
options(huxtable.knitr_output_format = "latex")
source("bayesian-heads.R")
suppressPackageStartupMessages({
  library(Hmisc)
  library(magrittr)
  library(dplyr)
  library(purrr)
  library(tidyr)
  library(readr)
  library(ggplot2)
  library(broom)
  library(huxtable)
})

huxtable::set_default_properties(position = "left", top_padding = 12, bottom_padding = 6)
set.seed(19751027)

nreps <- 1000L

# set to FALSE to recalculate all results; TRUE for speed:
cache_results <- FALSE  

# set to TRUE to (re)create the simulations CSV file:
rewrite_csv <- FALSE

# if TRUE, the number of liars in each simulation is randomly drawn
# from a population with `Liars` liars.
# If FALSE, the number of liars is fixed at `Liars * N`.
# If you change this, you may need to recreate the simulations CSV file
sample_from_pop <- FALSE 

# if FALSE, use L/N as ground truth;
# if TRUE, use (R-heads)/(N-heads):
estimate_lies_not_liars <- FALSE
                                
if (sample_from_pop && estimate_lies_not_liars) {
  stop("Can't estimate proportion of actual lies in population!")
}

params <-   list(
  N     = c(10, 50, 100, 500),
  Liars = seq(0, 1, 0.1),
  P     = c(.2, .5, .8)
)
 
ci_levels <- c(90L, 95L, 99L)

method_used_statement <- paste0("This paper can be recreated from the .Rmd file using different ",
      "methods to calculate confidence interval coverage and squared errors. 'Ground ",
      "truth' can be the population proportion of liars, the sample proportion of liars or ",
      "the sample proportion of actual lies. Ground truth for the document you are reading was: ",
        if (sample_from_pop) "the population proportion of liars." else 
        if (estimate_lies_not_liars) "the sample proportion of actual lies." else 
        "the sample proportion of liars."
      )

if (! sample_from_pop && ! estimate_lies_not_liars) method_used_statement <- ""

```


TODO XXX: point out the claims they make. Add your sims of when the posterior is wrong
and intervals fail even for (R-T)/(N-T).

**`r method_used_statement`**

Some people are honest, while others will lie whenever it benefits them. We would like to understand
the prevalence of lying, because dishonesty may be economically and socially harmful. Since we
cannot simply ask people if they are liars, one way to estimate the proportion of liars in a group
is to ask them to report the result of a coin flip or other random device, offering them a payment
if they report heads. Liars don't always lie: they only lie when it benefits them. So they always
report heads irrespective of the true coin flip. ^[GSV maintain this assumption and so shall I.] If
there are many more heads than we would expect by chance, we can assume many people are lying. But
how many?

A na誰ve estimate would be that if e.g. 80 people of 100 report heads, then on average 50 really saw
heads and 60% (30/50) of the remainder are lying. More generally, from *R* reports of a good outcome
in a sample of size *N*, where the _bad_ outcome happens with probability *P*, we can estimate that
a proportion $\frac{R/N - (1-P)}{P}$ are lying [@abeler2016preferences]. The problem with this approach is that the number of heads is not
fixed. If we see 1 out of 3 people reporting heads, this method estimates there are less than zero
liars. But it is still possible that everyone saw heads and 1 person lied.

@Garbarino2018 -- GSV from here on -- point out this problem and introduce an alternative method.
They claim that their method corrects for this problem and can estimate the full distribution of
lying outcomes, and they recommend using it for confidence intervals, hypothesis testing and power
calculations.

Table 3 of GSV reports Monte Carlo simulations, which show that the probability of a Type II error
can be up to 50 per cent. GSV define a Type II error as the 95% confidence interval not containing
the true value. A 95% confidence interval which only contains the true value 50% of the time
is unusual. What is going on?

```{r generate-data, cache = cache_results}

sim_data <- tidyr::crossing(
  N     = params$N,
  Liars = params$Liars,
  P     = params$P
)
sim_data$Liars_pop_pct <- sim_data$Liars


sim_data <- sim_data[rep(1:nrow(sim_data), each = nreps), ]
sim_data$sim <- rep_len(1:nreps, nrow(sim_data))

if (sample_from_pop) {
  sim_data %<>% 
        group_by(N, Liars, P) %>% 
        mutate(
          Liars_sample = rbinom(n(), N, Liars)
        ) %>% 
        ungroup() %>% 
        mutate(
          Liars = Liars_sample,
          Liars_sample = NULL
        )
} else { 
  sim_data$Liars <- round(sim_data$Liars * sim_data$N, 0)
}


simulate_cointoss <- function (row) {
  h_liars <- rbinom(1, row["Liars"], 1 - row["P"])
  h_honest <- rbinom(1, row["N"] - row["Liars"], 1 - row["P"])
  R <- row["Liars"] + h_honest
  names(R) <- NULL
  heads <- h_honest + h_liars
  c(R = R, heads = heads)
}

results <- apply(sim_data, 1, simulate_cointoss)

sim_data$R <- results["R", ]
sim_data$heads <- results["heads",]

sim_data <- sim_data %>% arrange(N, Liars, P)

nci_levels <- length(ci_levels)
sim_data <- sim_data[rep(seq_len(nrow(sim_data)), each = nci_levels), ]
sim_data$CI <- rep(ci_levels, nrow(sim_data)/nci_levels)

sim_data$P <- sim_data$P * 100 # input has to be in %

# algorithm is deterministic so we save time by 
# only printing out distinct lines
sim_output <- sim_data %>% 
      select(N, R, P, CI) %>% 
      distinct()

if (rewrite_csv) {
  path <- paste0("GSV-sims", if (sample_from_pop) "-pop-sample", ".csv")
  
  readr::write_csv(sim_output, path = path)

  # the java only lets you select a document from your home directory
  copied_to_home_dir_ok <- file.copy(path, normalizePath("~"), overwrite = TRUE)
  stopifnot(copied_to_home_dir_ok)
  # now run java -jar LyingCalculator.jar from the LyingCalculator directory
  # make sure the old output file (xxxOutput.csv) is deleted first, or it will be appended to!
}

sim_est <- sim_data
```

```{r sanity-checks, include = FALSE, cache = cache_results}
sim_data %<>% mutate(
  expected_R = Liars + (1 - P/100) * (N - Liars)
)
summary(lm(R ~ expected_R, sim_data))
sim_data %>% 
      group_by(P, N) %>% 
      do(
        broom::tidy(lm(R ~ expected_R, .))
      ) %>% 
      filter(term == "expected_R")

sim_data$expected_R <- NULL

```

```{r gather-estimates, cache = cache_results}

gsv_estimates <- suppressMessages(readr::read_csv(normalizePath("~/sim-outputOutput.csv")))

sim_est <- left_join(sim_est, gsv_estimates, by = c("N", "R", "P", "CI"))

# work with probs not percentages:
sim_est %<>% 
      mutate(
        LB = LB /100,
        UB = UB / 100,
        EV = EV / 100
      ) %>% 
      rename(
        LB.gsv = LB,
        UB.gsv = UB,
        EV.gsv = EV
      )
```

```{r bayesian-estimates, cache = cache_results}

my_prior <- uniform_prior
# alternative:
# my_prior <- function (N) dbinom(0:N, N, p = 0.5)

bayes_est <- sim_output %>% 
      rowwise() %>% 
      do({
        pstr <- update_prior(
                heads = .$R, 
                N = .$N, 
                P = .$P/100, 
                prior = my_prior(.$N)
              )
        ci <- .$CI / 100
        ci_qs <- c(1/2 - ci/2, 1/2 + ci/2)
        cis <- posterior_quantile(ci_qs, pstr)
        # calculate quantities as prop dishonest
        tibble(
          EV.ubayes = posterior_mean(pstr)/.$N,
          LB.ubayes = cis[1]/.$N, 
          UB.ubayes = cis[2]/.$N
        )
      })
bayes_est <- cbind(sim_output, bayes_est)
sim_est <- left_join(sim_est, bayes_est, by = c("N", "P", "R", "CI"))

```

```{r simple-estimates, cache = cache_results}

heads_to_liars <- function (prop_heads, p) {
  # excess good outcomes reported over what you'd expect: prop_heads - (1 - p)
  # over the expected proportion of outcomes that were bad: p
  # if there's no excess, assume everyone is honest
  pmax(0, (prop_heads - (1 - p)) / p)
}

simple_est <- sim_output %>%
      mutate(
        EV.simple = heads_to_liars(R/N, P/100)
      ) 

heads_confints <- apply(sim_output, 1, function (row) {
  ci <- binom.test(row["R"], row["N"], conf.level = row["CI"]/100)$conf.int
  res <- heads_to_liars(ci, row["P"]/100)
  names(res) <- c("LB.simple", "UB.simple")
  res
})

simple_est <- cbind(simple_est, t(heads_confints))
sim_est <- left_join(sim_est, simple_est, by = c("N", "P", "R", "CI"))
```

```{r recalculate-gsv-functions}

dist_L_gsv <- function(heads, N, P) {
  # L liars means that T = heads - L
  # T is distributed binomially conditional on being no more than `heads`
  L = 0:heads
  result <- rep(NA_real_, length(L))
  result <- dbinom(heads - L, N, 1 - P) / sum(dbinom(L, N, 1 - P))

  result
}

prop_liars <- function(L, heads, N) {
  stopifnot(all(L <= heads), heads >= 0, heads <= N)
  # for any number of liars L, reported heads, and N:
  # true heads T = heads - L
  # number observing low outcome = N - T
  # proportion lying is L/(N-T)
  T <- heads - L
  prop_lied <- L/(N - T)
  prop_lied[N == T] <- 1 # if everyone saw true heads, then   
                         # we can't estimate how many might have
                         # GSV code solves this by assuming 1

  prop_lied
}

prop_liars_ci_gsv <- function (ci, heads, N, P) {
  stopifnot(0 <= ci, ci <= 1, heads <= N, 0 <= heads, 0 <= P, P <= 1)
  
  dist_L <- dist_L_gsv(heads, N, P)
  check_posterior(dist_L)
  stopifnot(length(dist_L) == heads + 1)
  
  pl <- prop_liars(0:heads, heads, N)
  stopifnot(length(pl) == heads + 1)
  
  pl <- pl[dist_L > 0] # get rid of 0-probability events
  dist_L <- dist_L[dist_L > 0]
  dist_L <- dist_L[order(pl)]
  pl <- sort(pl)
  
  cdf <- cumsum(dist_L)
  
  # we know pl is always positive, so represent that:
  cdf <- c(0, cdf)
  pl  <- c(0, pl)
  
  tail <- (1 - ci)/2
  # lower bound: no more than tail% of probability has less than this
  lb_idx <- max(which(cdf <= tail)) # must exist since cdf starts at 0.
  lb <- pl[lb_idx]
  
  # upper bound: no more than 1-tail% of probability has more
  ub_idx <- min(which(cdf >= 1 - tail))
  ub <- pl[ub_idx]
  
  return(c(lb, ub))
}

prop_liars_ev_gsv <- function(heads, N, P) {
  dist_L <- dist_L_gsv(heads, N, P)
  prop_lied <- prop_liars(0:heads, heads, N)
  sum(dist_L * prop_lied)
}
```

```{r check-gsv-posterior, eval = FALSE}

# if we assume that liars are not uniformly distributed, the posterior of number of heads
# is not like the prior
N <- 10
CI <- 0.9
tmp <- replicate(10000, {
  # L <- sample(1:N, 1) # for this, always accurate?
  L <- rbinom(1, N, 0.1) # not many liars
  HL <- rbinom(1, L, 0.5)
  HNL <- rbinom(1, N-L, 0.5)
  R <- L + HNL
  H <- HL + HNL
  c(L, R, H, N, CI, prop_liars_ci_gsv(CI, R, N, 0.5))
})

tmp <- as.data.frame(t(tmp))
names(tmp) <- c('L', 'R', 'H', 'N', 'CI', 'cilo', 'cihi')
tmp$lies_told <- with(tmp, (R-H)/(N-H))
tmp$lies_in_ci <- tmp$cilo <= tmp$lies_told & tmp$lies_told <= tmp$cihi

# the prior:
ggplot(tmp, aes(H)) + geom_bar(aes(y = calc(prop))) + 
  geom_line(data = tmp[tmp$R > quantile(tmp$R, 0.8), ], aes(H, calc(prop)))
# the posterior is not the prior:
hist(tmp$H, col = "grey", breaks = 0:N, freq = FALSE, ylim = c(0, 1))
hist(tmp$H[tmp$R==10], breaks = 0:N, add= T, freq = FALSE, col = rgb(1,0,0,.5))

# the posterior:
lines(density(with(tmp, H[R > quantile(R, 0.9)])))

# how accurate are we for different numbers of R.
# NB you can also judge accuracy by different numbers of H, but I'm not sure that's fair as H is
# an unknown parameter - maybe it is like saying "confidence interval coverage is way off when
# the errors are big!"

GSV_coverage <- tmp %>% group_by(R) %>% 
  summarize(n_sims = n(), coverage = mean(lies_in_ci))

tmp %<>% left_join(GSV_coverage, by = "R")

scale_fac <- max(tmp$n_sims)
ggplot(GSV_coverage, aes(R)) + 
    geom_hline(yintercept = CI, linetype = 2) +
    geom_line(aes(y = coverage), colour = "red") +
    geom_line(aes(y = n_sims/scale_fac), colour = "grey") +
    scale_y_continuous(sec.axis = sec_axis(~.*scale_fac, name = "N sims"))
      
plot(GSV_coverage$R, GSV_coverage$coverage, type = "l")
# number of cases:
points(GSV_coverage$R, GSV_coverage$n_sims/max(GSV_coverage$n_sims), type = "h", col = "grey")

table(low_coverage = tmp$coverage < 0.8, tmp$R > 60) %>% prop.table(2)
```

```{r recalc-estimates, cache = cache_results}

sim_tmp <- sim_output[, c("R", "N", "P", "CI")]
recalc <- apply(sim_tmp, 1, function (x) {
  x <- c(
    prop_liars_ev_gsv(x["R"], x["N"], x["P"]/100),
    prop_liars_ci_gsv(x["CI"]/100, x["R"], x["N"], x["P"]/100)
  )
  names(x) <- paste0(c("EV", "LB", "UB"), ".recalc")
  x
}) 
sim_tmp <- cbind(sim_tmp, t(recalc))
sim_est <- left_join(sim_est, sim_tmp, by = c("N", "P", "R", "CI"))

```

```{r reshape-results, cache = cache_results}

# reshape to long by method
sim_est %<>%
      tidyr::gather(measure, value, matches("^(LB|EV|UB)\\.")) %>% 
      tidyr::extract(measure, into = c("statistic", "method"), regex = "^(LB|EV|UB)\\.(.*)") %>% 
      tidyr::spread(statistic, value) 


# alternative "ground truths":
if (estimate_lies_not_liars) {
  sim_est %<>% mutate(
          true_prop = (R - heads)/(N - heads)
        )
} else if (sample_from_pop) {
  sim_est %<>% mutate(
          true_prop = Liars_pop_pct
        )
} else {
  sim_est %<>% mutate(
          true_prop = Liars/N
        )
}

sim_est %<>% mutate(
        within_ci = true_prop >= LB & true_prop <= UB,
        error     = EV - true_prop,
        error_sq  = error^2
      )

# only use recalc in appendix
sim_est_recalc <- sim_est %>% filter(method %in% c("gsv", "recalc"))
sim_est <- sim_est %>% filter(method != "recalc")
```


I ran simulations to check the overall performance of the GSV confidence intervals. Simulations
parameters are shown in Table \ref{tab:params-table}. For each parameter combination, I ran 1000
simulations. "Liars" always reported heads. Non-liars reported tails with probability *P* and
heads otherwise. For each simulation and confidence interval, I computed whether the confidence interval 
contained the true value of *L/N*. Table \ref{tab:coverage-overall} shows the results.


```{r params-table}

huxtable(
        Parameter = c(
          "Sample size (N)", 
          "Proportion of liars (L/N)", 
          "Probability of bad random outcome (P)"
        ),
        Values    = c(
          paste(params$N, collapse = ", "),
          paste(params$Liars, collapse = ", "),
          paste(params$P, collapse = ", ")
        ),
        add_colnames = TRUE
      ) %>% 
      set_caption("Parameter values") %>% 
      set_label("tab:params-table") %>% 
      theme_plain()
```



```{r coverage-overall}

coverage <- sim_est %>% 
      group_by(method, CI) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE))

cov_overall <- coverage %>% 
      ungroup() %>% 
      mutate(
        CI = paste0("CI ", CI, "%"),
        Method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian"),
        method = NULL
      ) %>% 
      spread(CI, mean_in_ci) %>% 
      arrange(factor(Method, levels = c("GSV", "Frequentist", "Bayesian"))) 

pct_format <- function (x) sprintf("%.1f%%", x * 100)

ht_overall <- as_huxtable(cov_overall, add_colnames = TRUE) %>% 
      set_caption("Proportion of true results within confidence interval.") %>% 
      set_label("tab:coverage-overall") %>% 
      set_number_format(-1, -1, list(pct_format)) %>% 
      theme_plain()


ht_just_gsv <- ht_overall[1:2,]
ht_just_gsv[1, 1] <- ""
theme_plain(ht_just_gsv)
```

By definition, 95% of 95% confidence intervals ought to contain the true value, on average. This
is called "achieving nominal coverage". GSV
intervals are far from this. The confidence intervals are too narrow. Further analysis shows that
this problem holds across all simulated probabilities of the low outcome *P*, confidence
levels, and sample sizes *N* (up to 500).^[See the appendix.]

Why does this happen? We can get a clue by running the GSV method when there are 10 reports of 
"heads" out of 10 for a fair coin flip (*R = N = 10, P = 0.5*). The resulting point estimate is that
100% of subjects lied. The upper and lower 99% confidence intervals are also 100%. Na誰vely, this might
seem wrong. Can we rule out that 8 subjects lied, and the two honest subjects both saw heads?

In fact, the GSV method works like this. First, given *R* reports of heads, the probability that a total 
of *T* "true" heads were observed is calculated as:

\[
Z_{T,R,N,p} = \frac{
  \textrm{binom}(T,N,p)
}{
  \sum^R_{k=0}\textrm{binom}(k,N,p)
}
\]

This is the binomial distribution, truncated at *R*, because by assumption, nobody "lies downward"
and reports tails when they really saw heads.

Next, from *T* the number of liars is calculated as *R-T*; and the proportion of liars is calculated
as *(R-T)/(N-T)*, because *N-T* people saw the low outcome and had the chance to lie. Combining this
with the truncated binomial gives a cumulative distribution function of the proportion of liars.
This can then be used to estimate means and confidence intervals.

Putting these together, for *R = N = 10*, the estimated distribution of liars is calculated as follows:

* With probability $\frac{1}{1024}$, there were really *T = 10* heads. Nobody lied in the sample.^[But  the proportion of people who lied *out of those who saw tails* is undefined, because noone saw tails. The 
GSV software seems to resolve this by fixing the proportion of lies to 100%.] 

* Otherwise, 1 or more people saw tails, and they all lied. The proportion of liars is 100%.

Hence, the lower and upper confidence intervals are all 100%.

There are two problems with this approach. One is statistical, one is conceptual.

First, if 10 out of 10 heads are reported, you should learn two things. On the one hand, there
are probably many liars in your sample. On the other hand, probably a lot of coins really landed heads.
The probability distribution above does not take account of this. In particular, for *R = N*, 
the equation above reduces to the binomial distribution. This is not correct. 

Second, more importantly, the GSV approach estimates the distribution of the proportion of 
*lies actually told*, among the subsample of people who saw tails:

\[
\textit{Lies} = \frac{R-T}{N-T} 
\]

But usually, we are not interested in the proportion of
lies actually told. We care about how many people in the sample *are liars*, who would lie if they
saw tails:

\[
\textit{Liars} = \frac{L}{N}.
\]

We care about *Liars* not *Lies*, because *Lies* is just a noisy estimate of *Liars*: it is the
number of liars who happened to see heads. It is a function of honesty and luck. We are interested
in the honesty.^[There may be times when social scientists care about how many people actually
lied, but I cannot think of many.]

Suppose *R = 10* heads out of 10 are reported, and suppose 9 heads were really observed, *T = 9*.
*Lies* is indeed 100%.  But it is 100% of 1 person
-- the only person who saw tails. That one person's behaviour is an unbiased estimate of the
proportion of liars in the sample, *Liars*. But it is a very noisy estimate.

The first problem means that when *R* is very high or low,
the percentage of confidence intervals containing *Lies*
may fall off sharply.^[Depending on the distribution of the number of liars *L*.]

The second problem means that the percentage of confidence intervals containing *Liars*
is almost always much less than the nominal coverage. 

```{r disagg-coverage}
disagg <- sim_est %>% 
      filter(method=="gsv") %>% 
      group_by(
        `R/N:` = R/N, N, P, CI
      ) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE), prop = n()/nrow(.)) %>% 
      mutate(CI = paste0("CI ", CI, "%")) %>% 
      spread(CI, mean_in_ci) %>% 
      mutate(ok  = `CI 95%` > 0.95)

prop_ok <- disagg %>%  
      ungroup() %>% 
      summarize(prop_ok = sum(prop[ok])) %>% 
      pull(prop_ok)
```

The problem is especially bad when there are many reports of heads. In this case there were probably
many true heads, so *T* is high and the true sample size *N - T* is low. Table \ref{tab:coverage-rn}
shows this. It splits the simulations by the proportion of reported heads, *R/N*. Coverage levels
fall off sharply as *R/N* increases. Note that for fair coin flips, *R/N* is typically greater than
0.5, in the simulations and in reality.

```{r coverage-rn}
coverage_rn <- sim_est %>% 
      filter(method=="gsv") %>% 
      group_by(
        `R/N:` = factor(Hmisc::cut2(R/N, cuts = c(0, 1/4, 1/2, 3/4, 1)))      
      ) %>% 
      mutate(pct_cases = n()/nrow(.)) %>% 
      group_by(`R/N:`, CI) %>% 
      summarize(
        mean_in_ci  = mean(within_ci, na.rm = TRUE), 
        `Percentage of cases` = pct_cases[1]
      ) %>% 
      mutate(CI = paste0("CI ", CI, "%")) %>% 
      spread(CI, mean_in_ci)
    

as_huxtable(coverage_rn, add_colnames = TRUE) %>% 
      set_caption("GSV confidence interval coverage by low and high R/N") %>% 
      set_label("tab:coverage-rn") %>% 
      set_escape_contents(2:3, 1, FALSE) %>% 
      set_number_format(-1, -1, list(pct_format)) %>% 
      theme_plain()
```

# Alternative methods

Can we do better? There are two alternative methods one might use to calculate confidence intervals.
The first is simply to calculate confidence bounds for the proportion of
heads that would be reported in the population, given the number that were reported in the sample. 
One can then back out
confidence bounds for the proportion of liars as $\frac{R/N - (1-P)}{P}$, as in
@abeler2016preferences.^[Confidence bounds lower than 0 can be set to 0.]  As GSV point out, this
assumes that the sample proportion of high outcomes was equal to its expected proportion. We'll see 
whether this matters.

There are numerous ways to calculate confidence intervals in a test of proportions. See e.g.
@agresti1998approximate. Here, I use the binomial exact test of @clopper1934use, which is 
known to be conservative. I call this the "frequentist" method.

The second method is to use Bayes' rule. Start with a prior probability distribution over
the number of liars *L*, *Pr(L)*. The posterior probability is then:

\[
\textrm{Pr}(L | R; N,P) = \frac{
  \textrm{Pr}(R|L; N,P) \textrm{Pr}(L)
}{
  \sum_{L'=0}^N \textrm{Pr}(R|L'; N,P) \textrm{Pr}(L')
}
\]

where the probability of getting *R* reports of heads in total for given *L* is just
the probability that the *N - L* honest people saw *R - L* heads:

\[
\textrm{Pr}(R|L; N,P) = \textrm{binom}(R - L, N - L,1 - P)
\]

One can then estimate the proportion of *Liars* as *L/N*. From
this, one can derive confidence intervals and expected values in the usual way.

For the estimation here, I used a uniform prior, with *Pr(L) =* $\frac{1}{N+1}$ for *L* in  0
to *N*.

I used these estimation methods on my simulated data. Results are shown in Table
\ref{tab:coverage-allmethods}. Both frequentist and  Bayesian methods mostly achieve the nominal
confidence level, with more than 90/95/99% of intervals containing the true value of *Liars*. The
exception is the frequentist 99% confidence interval, which is too narrow.

```{r coverage-allmethods}
ht_overall %>% 
      set_caption("Coverage levels for GSV and alternative methods") %>% 
      set_label("tab:coverage-allmethods")
```

Frequentist confidence intervals could be less accurate when N is low, since that leads to more
sampling variation in the number of true heads. Table \ref{tab:coverage-N} checks this by looking
separately at simulations with *N = 10* and *N = 50*. Frequentist 99% confidence intervals indeed
appear slightly too narrow for this range. Bayesian confidence intervals are fine. Both alternatives
perform much better than the GSV approach.


```{r coverage-N}

coverage_N <- sim_est %>% 
      filter(N <= 50) %>% 
      group_by(method, CI, N) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        CI = paste0("CI ", CI, "%"),
        Method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian"),
        method = NULL
      ) %>% 
      select(Method, N, CI, mean_in_ci) %>% 
      spread(CI, mean_in_ci) %>% 
      arrange(factor(Method, levels = c("GSV", "Frequentist", "Bayesian"))) 
  

as_huxtable(coverage_N, add_colnames = TRUE) %>% 
      set_rowspan(c(2, 4, 6), "Method", 2) %>% 
      set_valign(c(2, 4, 6), "Method", "top") %>% 
      set_number_format(-1, contains("CI"), list(pct_format)) %>% 
      set_caption("Confidence interval coverage by sample size") %>% 
      set_label("tab:coverage-N") %>% 
      theme_plain() %>% 
      set_bottom_border(6, 1, 0.4)
```


# Point estimation


We can also compare the accuracy of point estimates of *Liars*. Table
\ref{tab:point-estimates} shows the mean squared error for methods by different *N*. For low *N*,
the best method is Bayesian and the worst is Frequentist, with GSV in between. When *N* gets large all
methods give about the same estimates and are equally accurate. 

The Bayesian method might have an advantage here, since it assumes a uniform prior and the
simulations indeed used a uniform distribution of the proportion of liars *L/N*. In fact, further
analysis reveals that the Bayesian method is best across all specific values of *L/N* up to
80%.^[When *L/N = 1*, all subjects deterministically report heads, and both Frequentist and
Bayesian point estimates are exactly correct.] So, the Bayesian method is likely to be best unless
one is sure that the true *L/N* is rather high.

```{r point-estimates}

sim_est %>% 
      filter(CI == 90) %>% 
      group_by(method, N) %>% 
      summarize(mean_sq_error = mean(error_sq, na.rm = TRUE)) %>% 
      spread(N, mean_sq_error, sep = ": ") %>% 
      ungroup() %>% 
      mutate(
        method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian")
      ) %>% 
      as_huxtable(add_colnames = TRUE) %>% 
      set_caption("Mean squared errors by method and N") %>% 
      set_label("tab:point-estimates") %>% 
      theme_plain()
```


# Empirical Bayes

Bayesian estimates are accurate, but rely on a choice of prior.
A non-informative prior is a reasonable choice. Alternatively one might use information
from previous meta-analyses such as @abeler2016preferences. If the sample size is large enough,
the choice of prior should not matter much.

When comparing the dishonesty rates of different groups, a possible approach is to use the
"empirical Bayes" method [@casella1985introduction]. This means that one estimates a common prior
from the pooled data, before updating the prior for each individual group.


```{r hj-data}

load("honesty.RData")
dfr <- dfr %>% 
      filter(heads %in% 1:2, ! is.na(resnat) , age %in% 2:6)

nats <- dfr %>%  
      group_by(resnat) %>% 
      summarize(
        heads = sum(heads==1), 
        N = n(),
        prop_heads = heads/N,
        prop_liars = 2*prop_heads - 1
      )
pooled_prop_liars <- mean(dfr$heads == 1) * 2 - 1

distribs <- apply(nats[, c("heads", "N")], 1, function (r) {
  N <- r["N"]
  update_prior(
      heads = r["heads"],
      N = N,
      P = 0.5, 
      prior = dbinom(0:N, N, p = pooled_prop_liars)
  )
})


names(distribs) <- nats$resnat

nat_means <- sapply(distribs, posterior_mean)
nat_cis <- sapply(distribs, posterior_quantile, q = c(0.025, 0.975))
rownames(nat_cis) <- c("ci.low", "ci.high")

nats <- cbind(nats, t(nat_cis), mean = nat_means)

nats %<>% 
      mutate(
        mean = mean/N,
        ci.low = ci.low/N,
        ci.high = ci.high/N
      ) %>% 
      arrange(mean)

# nats$resnat <- factor(nats$resnat, levels = nats$resnat)
```

For example, @hugh2016honesty estimates the dishonesty rates of 15 nations using a coin flip
experiment. The pooled estimate of the proportion of liars in the sample is given by *2R/N - 1* = 
`r round(pooled_prop_liars, 2)`. With *N* = `r nrow(dfr)`, this will not be much affected by 
sampling variation in the number of heads. We can use a binomial distribution with this as the prior.

```{r hj2016-pic, fig.cap = "Posterior means and confidence intervals for proportion dishonest in 15 countries. Crosses are na誰ve estimates.", fig.align="center", fig.width=5.5, fig.height=4}

ggplot(nats, aes(x = factor(resnat, levels = resnat), y = mean, fill = mean)) +
      geom_col(alpha = 0.8) +
      geom_linerange(aes(ymin = ci.low,ymax = ci.high)) +
      geom_point(aes(y = prop_liars), shape = 4, size = 3) +
      xlab("Nation") + ylab("Prop. liars") +
      viridis::scale_fill_viridis(option = "E", guide = FALSE) + 
      theme_light() +
      coord_flip()

```


Results are shown in Figure \ref{fig:hj2016-pic}, with means and 95% confidence intervals. There
is substantial shrinkage from the "na誰ve" per-country estimates found by calculating *2R/N - 1*
separately for each country). One of the strengths of empirical Bayes,  as @casella1985introduction
points out, is that it "anticipates regression to the mean". That is, as there is noise in the data,
the most and least honest groups in the data probably had some extreme error terms. So the
British people in the sample may be less honest, and the Chinese people more honest, than the per-group 
data suggest.

# Bayesian hypothesis testing

We can also test hypotheses using the Bayesian approach. If two samples are independent, then the 
probability that e.g. the true proportion of liars in sample 1 is smaller than in sample 2 can be
calculated from the marginal distributions in each sample:

\[
\sum_{L = 0}^{N_{1}} \sum_{M=0}^{N_{2}} Pr(L_{1} = L) Pr(L_{2} = M) \;
  \mathbb{1}({\frac{L}{N_{1}} < \frac{M}{N_{2}})}
\]

Table \ref{tab:country-comparisons} shows Bayesian comparisons for selected countries. Even though
estimates have "shrunk" towards the pooled mean from the naive estimates, differences remain
significant, partly because the pooled prior has made posterior confidence intervals narrower.

This approach has a nice interpretation, analogous to the use of the null hypothesis in frequentist
methods. We start from the belief that the proportion of liars is the same across all groups. The
posterior then shows how much the evidence changes this belief.

```{r country-comparisons}

prob_country_less <- function (ctry1, ctry2) {
  stopifnot(ctry1 %in% nats$resnat, ctry2 %in% nats$resnat)
  N_1 <- nats$N[nats$resnat == ctry1]
  N_2 <- nats$N[nats$resnat == ctry2]
  heads_1 <- nats$heads[nats$resnat == ctry1]
  heads_2 <- nats$heads[nats$resnat == ctry2]  
  diff_dist <- compare_groups(heads1 = heads_1, N1 = N_1, heads2 = heads_2, N2 = N_2, P = 0.5)
 
  prob_first_less <-sum(diff_dist$prob[diff_dist$diff < 0]) 
  return(prob_first_less)
}

ctry_comp <- outer(nats$resnat, nats$resnat, Vectorize(prob_country_less))
diag(ctry_comp) <- NA
rownames(ctry_comp) <- nats$resnat
colnames(ctry_comp) <- nats$resnat

selected <- c("GB", "ZA", "GR", "BR", "CN")
ctry_comp <- ctry_comp[selected, selected]
cc_hux <- as_huxtable(ctry_comp, add_colnames = TRUE, add_rownames = TRUE) %>% 
      set_caption("Bayesian country comparisons. 
            Cells give the probability that row country has fewer liars than column country.") %>% 
      set_label("tab:country-comparisons") %>% 
      set_na_string("--") %>% 
      set_number_format("%.3f") %>% 
      set_align(-1, -1, ".") %>% 
      theme_plain() %>% 
      set_bold(everywhere, 1, TRUE)

cc_hux[1, 1] <- ''

cc_hux
```


# The population

So far we have been trying to estimate the proportion of *Liars* in the sample. In fact,
we usually care about the proportion of liars in populations from which the sample is drawn. This
adds another layer of randomness. When *N* is large, the sample distribution should approximate
the population distribution and either frequentist or Bayesian approaches should work. Further
simulations, not shown, suggest that Bayesian confidence intervals are less robust than frequentist ones
in bounding the population proportion of *Liars* when the sample is randomly drawn from the population.

# Conclusion

These results suggest some simple recommendations when you are analysing a coin-flip
style experiment.

1. Do not use the GSV confidence intervals, unless you are specifically interested in *Lies* rather
  than *Liars*. Even in that case, be aware that the confidence intervals have low coverage for extreme
  values of *R*.
2. If your N is reasonably large, say at least 100, you can safely use standard frequentist 
  confidence intervals and tests. If you are interested in the population from which the sample is drawn,
  you should do so, as they are most robust.
3. If your N is small and you are interested in the sample proportion of *Liars*, consider
  Bayesian estimates. To estimate differences between subgroups, consider
  empirical Bayes with a prior derived from the pooled sample.

Lastly, here is a simple way to avoid these shenanigans. Ask a group of *N* experimental subjects, 
not to flip a coin, but to draw a black or white ball, without replacement, from an urn containing 
*N* balls, of which *T* are white. Pay them if they report a white ball. You will then know -- not
estimate -- that the proportion of lies told is *(R-T)/(N-T)*, and this will also be the proportion
of liars, in a sample of *N-T* subjects. So you can simultaneously estimate the proportion of
*Lies*, and of the lying *Liars* who tell them.

\FloatBarrier
\newpage

# Appendix

R code to reproduce this comment is available at https://github.com/hughjonesd/GSV-comment. You
can also rerun the simulations to test confidence interval coverage of *Lies*, of *Liars*, or
of *Liars* in a population from which samples are drawn. Lastly, I provide code to find Bayesian
posterior distributions of *Liars*.

The errors in GSV confidence intervals could be due to a programming error rather than to the
algorithm. I could reproduce GSV's expected value to 4 or 5 significant figures by following their
method, but I could not reproduce their confidence intervals. For example, 
when *R = 3, N = 6, P = 0.5*, GSV's Java program gives the upper bound of the 95% confidence 
interval for the proportion of liars as 49.91%. But the possible proportions of liars when there
are *T=0,1,2,3* true heads, are *(R-T)/(N-T) =* 
`r paste0(prop_liars(3:0, 3, 6) * 100, "%", collapse = ", ")`.
It may be that some linear interpolation is being done. 

Nevertheless, if I use GSV's method as stated, rather than their program, confidence intervals
remain too small, as shown in Table \ref{tab:coverage-recalc}.

```{r coverage-recalc}

coverage_recalc <- sim_est_recalc %>% 
      filter(method == "recalc") %>% 
      group_by(CI) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        CI = paste0("CI: ", CI, "%"),
      ) %>% 
      spread(CI, mean_in_ci) 

as_huxtable(coverage_recalc, add_colnames = TRUE) %>% 
      set_caption("Proportion of true results within confidence interval, recalculated GSV method") %>% 
      set_label("tab:coverage-recalc") %>% 
      set_number_format(-1, everywhere, list(pct_format)) %>% 
      theme_plain()

```

Figure \ref{fig:coverage-pic} shows the proportions of true values within the confidence interval
for the GSV method, split by *N*, *P* and confidence level. Dashed lines show the nominal confidence
level.

Figure \ref{fig:coverage-grp-pic} splits this still further, by the true proportion of liars within
each simulation. This makes the pattern clear: coverage gets worse as the proportion of liars
increases (until 100% where it jumps back up as results become deterministic).

```{r coverage-pic, fig.cap = "GSV confidence interval coverage by confidence level, N and P", fig.width=5.5, fig.height=4, fig.align="left"}


coverage_treat_gsv <- sim_est %>% 
      filter(method == "gsv") %>% 
      group_by(CI, N, P) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        P = factor(paste0(P, "%")),
        ci = CI,
        CI = paste0(CI, "%")
      )

ggplot(coverage_treat_gsv, aes(P, mean_in_ci, colour = P)) +
      geom_point(size = 2) +  
      geom_hline(aes(yintercept = ci/100), linetype = 2) +
      facet_grid(N ~ CI, labeller = "label_both") +
      xlab("Probability of low outcome (P)") + ylab("Proportion within C.I.")
```


```{r coverage-grp-pic, fig.cap = "GSV confidence interval coverage by confidence level, N, P and true proportion of liars", fig.align="left", fig.width=6, fig.height=4}

coverage_grp_gsv <- sim_est %>% 
      filter(method == "gsv") %>% 
      group_by(CI, N, Liars, P) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        P = factor(paste0(P, "%")),
        ci = CI,
        CI = paste0(CI, "%")
      )

g <- ggplot(coverage_grp_gsv, aes(Liars/N, mean_in_ci, colour = P)) +
      geom_hline(aes(yintercept = ci/100), linetype = 2) +
      facet_grid(N ~ CI, labeller = "label_both") +
      xlab("Liars") + ylab("Proportion within C.I.")

if (sample_from_pop) g + geom_smooth(method = "loess") else g + geom_point() + geom_line()

```


Figure \ref{fig:errors-by-true-prop} shows mean squared error by estimation method and 
true proportion of liars in the sample (*Liars*), for *N* of 10 and 50. The Bayesian method is best 
for all values of *Liars* up to 80%.

```{r errors-by-true-prop, fig.cap = "Errors by method and true proportion of liars, N = 10 and 50", fig.align="left", fig.width=5.5, fig.height=4}
errors_true_prop <- sim_est %>%       
      filter(CI == 90, N <= 50) %>% 
      group_by(method, true_prop) %>% 
      summarize(mean_sq_error = mean(error_sq, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian")
      ) %>% 
      rename(`Liars` = true_prop, Method = method)

g <- ggplot(errors_true_prop, aes(`Liars`, mean_sq_error, colour = Method)) + 
      ylab("Mean square error") + 
      scale_x_continuous(breaks = 0:5/5)

# proportion of liars is set to 0:10/10, but of lies is random,
# so we have to use a smoothing line
if (estimate_lies_not_liars) g + geom_smooth(method="loess") else 
      g + geom_line() + geom_point()

```

```{r errors-disagg, include = FALSE, fig.align="left", fig.width=5.5, fig.height=4, fig.cap="Point estimate errors by R/N, N and P"}

errors_disagg <- sim_est %>%       
      filter(CI == 90, N <= 50) %>% 
      group_by(rn = findInterval(R/N, 0:10/10, rightmost.closed = TRUE), N, P, method) %>% 
      summarize(mean_sq_error = mean(error_sq, na.rm = TRUE)) %>% 
      mutate(
        method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian")
      ) %>% 
      rename(`R/N` = rn, Method = method)

g <- ggplot(errors_disagg, aes(`R/N`, mean_sq_error, colour = Method)) + 
      ylab("Mean square error") + facet_wrap(~N + P, scales = "free_y")

if (estimate_lies_not_liars) g + geom_smooth(method="loess") else 
      g + geom_line() + geom_point()
```

\FloatBarrier

\newpage

# References
