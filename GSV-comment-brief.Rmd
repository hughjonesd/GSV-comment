---
title: "True Lies:"
subtitle: "Comment on Garbarino, Slonim and Villeval (2018)"
author: "David Hugh-Jones"
date: "`r Sys.Date()`"
header-includes:
  - \usepackage{setspace}
  - \usepackage{placeins}
  - \onehalfspacing
  - \usepackage{fontspec}
output:
  pdf_document:
    md_extensions: +raw_attribute
    latex_engine: xelatex
  tufte::tufte_handout:
    latex_engine: xelatex
    md_extensions: +raw_attribute
  bookdown::tufte_handout2:
    latex_engine: xelatex
    md_extensions: +raw_attribute
  bookdown::tufte_html2: 
    pandoc_args: ['--self-contained']
    keep_intermediates: true
bibliography: bibliography.bib
link-citations: yes
fontsize: 12pt
mainfont: Times New Roman
mathfont: Times New Roman
---


> Garbarino, Slonim and Villeval (2018) describe a new method to calculate the probability
> distribution of the proportion of lies told in "coin flip" style experiments. I show that their
> estimates and confidence intervals are flawed. I demonstrate two better ways to estimate the 
> probability distribution of what we are really interested in: the proportion of liars.

```{r setup, include = FALSE}

library(checkpoint)
checkpoint("2018-11-03")

library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE)
options(huxtable.knitr_output_format = "latex")

source("bayesian-heads-cts.R")
source("GSV-heads.R")

suppressPackageStartupMessages({
  library(Hmisc)
  library(MASS)
  
  library(dplyr)
  library(purrr)
  library(broom)
  library(readr)
  library(magrittr)
  library(tidyr)
  library(ggplot2)
  
  library(huxtable)
  library(hdrcde)
})

huxtable::set_default_properties(position = "left", top_padding = 12, bottom_padding = 6)
ggplot2::theme_set(theme_light())

set.seed(19751027)

# how many simulations to run:
nreps <- 1000L

# set to FALSE to recalculate all confidence interval estimates; TRUE for speed:
cache_results <- FALSE

# set to TRUE to (re)create the simulations CSV file, then stop:
rerun_java <- FALSE

params <-   list(
  N      = c(10, 50, 100, 500),
  lambda = seq(0, 1, 0.1),
  P      = c(.2, .5, .8)
)
 
ci_levels <- c(90L, 95L, 99L)

```


TODO XXX: point out the claims they make. 


Some people are honest, while others are likely to lie whenever it benefits them. We would like to understand
the prevalence of lying, because dishonesty may be economically and socially harmful. Since we
cannot simply ask people if they are liars, one way to estimate the proportion of liars in a group
is to ask them to report the result of a coin flip or other random device, offering them a payment
if they report heads. Liars don't always lie: they only lie when it benefits them. So they always
report heads irrespective of the true coin flip. ^[GSV maintain this assumption and so shall I.] If
there are many more heads than we would expect by chance, we can assume many people are lying. But
how many?

A naÃ¯ve estimate would be that if e.g. 80 people of 100 report heads, then on average 50 really saw
heads and 60% (30/50) of the remainder are lying. More generally, from *R* reports of a good outcome
in a sample of size *N*, where the _bad_ outcome happens with probability *P*, we can estimate that
the following proportion are lying [@abeler2016preferences]:

\begin{equation}
\label{eqn:naive-estimator}
\frac{R/N - (1-P)}{P}
\end{equation}
The problem with this approach is that the number of heads is not
fixed. If we see 1 out of 3 people reporting heads, this method estimates there are less than zero
liars. But it is still possible that everyone saw heads and 1 person lied.

@Garbarino2018 -- GSV from here on -- point out this problem and introduce an alternative method.
They claim that their method corrects for this problem and can estimate the full distribution of
lying outcomes, and they recommend using it for confidence intervals, hypothesis testing and power
calculations.

Table 3 of GSV reports Monte Carlo simulations, which show that the probability of a Type II error
can be up to 50 per cent. GSV define a Type II error as the 95% confidence interval not containing
the true value. A 95% confidence interval which only contains the true value 50% of the time
is unusual. What is going on?

```{r generate-data, cache = cache_results}

sim_data <- tidyr::crossing(
  N      = params$N,
  lambda = params$lambda,
  P      = params$P
)

sim_data <- sim_data[rep(1:nrow(sim_data), each = nreps), ]
sim_data$sim <- rep_len(1:nreps, nrow(sim_data))


sim_data %<>% mutate(
        heads = rbinom(nrow(.), N, 1 - P),
        lies = rbinom(nrow(.), N - heads, lambda),
        R    = heads + lies
      )

sim_data %<>% arrange(N, lambda, P)
  

nci_levels <- length(ci_levels)
sim_data <- sim_data[rep(1:nrow(sim_data), each = nci_levels), ]
sim_data$CI <- rep(ci_levels, nrow(sim_data)/nci_levels)


# algorithm is deterministic so we save time by 
# only printing out distinct lines
sim_input <- sim_data %>% 
      select(N, R, P, CI) %>% 
      distinct()

file_root <- "GSV-sims"
sim_output_file <- paste0(file_root, "Output.csv")
if (rerun_java) {
  sim_input_file <- paste0(file_root, ".csv")
  # input has to be in percentages:
  readr::write_csv(sim_input %>% mutate(P = P * 100), path = sim_input_file)

  # the java only lets you select a document from your home directory X-o
  copied_to_home_dir_ok <- file.copy(sim_input_file, normalizePath("~"), overwrite = TRUE)
  stopifnot(copied_to_home_dir_ok)
  stop(sprintf("Now run java -jar LyingCalculator.jar and input '%s'", sim_input_file),
        " from your home directory.\n",
        "Make sure the old output file is deleted first!\n",
        sprintf("The java will create a new file called '%s'.\n", sim_output_file), 
        "Leave it in your home directory, set `rerun_java` to FALSE and reknit this file.")
}

sim_est <- sim_data
```

```{r sanity-checks, include = FALSE, cache = cache_results}
sim_data %<>% mutate(
  expected_R = N * ((1 - P) + lambda * P)
)
summary(lm(R ~ expected_R, sim_data))
sim_data %>% 
      group_by(P, N) %>% 
      do(
        broom::tidy(lm(R ~ expected_R, .))
      ) %>% 
      filter(term == "expected_R")

sim_data$expected_R <- NULL

```

```{r gather-estimates, cache = cache_results}

gsv_estimates <- suppressMessages(readr::read_csv(normalizePath(file.path("~", sim_output_file))))

sim_est %<>% 
      mutate(p_pct = round(P * 100, 0)) %>% 
      left_join(gsv_estimates, by = c("N", "R", "p_pct" = "P", "CI")) %>% 
      mutate(p_pct = NULL)

# work with probs not percentages:
sim_est %<>% 
      mutate(
        LB = LB /100,
        UB = UB / 100,
        EV = EV / 100
      ) %>% 
      rename(
        LB.gsv = LB,
        UB.gsv = UB,
        EV.gsv = EV
      )
```

```{r bayesian-estimates, cache = cache_results}

my_prior <- dunif

bayes_est <- sim_input %>% 
      rowwise() %>% 
      do({
        pstr <- update_prior(
                heads = .$R, 
                N = .$N, 
                P = .$P, 
                prior = my_prior
              )
        cis <- suppressWarnings(dist_hdr(pstr, .$CI/100))
        # calculate quantities as prop dishonest
        tibble(
          EV.ubayes = dist_mean(pstr),
          LB.ubayes = cis[1], 
          UB.ubayes = cis[2]
        )
      })
bayes_est <- cbind(sim_input, bayes_est)
sim_est <- left_join(sim_est, bayes_est, by = c("N", "P", "R", "CI"))

```

```{r simple-estimates, cache = cache_results}

heads_to_liars <- function (prop_heads, p) {
  # excess good outcomes reported over what you'd expect: prop_heads - (1 - p)
  # over the expected proportion of outcomes that were bad: p
  # if there's no excess, assume everyone is honest
  pmax(0, (prop_heads - (1 - p)) / p)
}

simple_est <- sim_input %>%
      mutate(
        EV.simple = heads_to_liars(R/N, P)
      ) 

heads_confints <- apply(sim_input, 1, function (row) {
  ci <- binom.test(row["R"], row["N"], conf.level = row["CI"]/100)$conf.int
  res <- heads_to_liars(ci, row["P"])
  names(res) <- c("LB.simple", "UB.simple")
  res
})

simple_est <- cbind(simple_est, t(heads_confints))
sim_est <- left_join(sim_est, simple_est, by = c("N", "P", "R", "CI"))
```



```{r recalc-estimates, cache = cache_results}

sim_tmp <- sim_input[, c("R", "N", "P", "CI")]
recalc <- apply(sim_tmp, 1, function (x) {
  x <- c(
    prop_liars_ev_gsv(x["R"], x["N"], x["P"]),
    prop_liars_ci_gsv(x["CI"]/100, x["R"], x["N"], x["P"])
  )
  names(x) <- paste0(c("EV", "LB", "UB"), ".recalc")
  x
}) 
sim_tmp <- cbind(sim_tmp, t(recalc))
sim_est <- left_join(sim_est, sim_tmp, by = c("N", "P", "R", "CI"))

```

```{r reshape-results, cache = cache_results}

# reshape to long by method
sim_est %<>%
      tidyr::gather(measure, value, matches("^(LB|EV|UB)\\.")) %>% 
      tidyr::extract(measure, into = c("statistic", "method"), regex = "^(LB|EV|UB)\\.(.*)") %>% 
      tidyr::spread(statistic, value) 


sim_est %<>% mutate(
        within_ci = lambda >= LB & lambda <= UB,
        error     = EV - lambda,
        error_sq  = error^2
      )

# only use recalc in appendix
sim_est_recalc <- sim_est %>% filter(method %in% c("gsv", "recalc"))
sim_est <- sim_est %>% filter(method != "recalc")
```


I ran simulations to check the overall performance of the GSV confidence intervals. Simulations
parameters are shown in Table \ref{tab:params-table}. For each parameter combination, I ran 1000
simulations. $\lambda$ is the probability that an individual in the sample lies and report heads
when they observe tails:

\begin{equation}
\label{eqn:lambda}
\lambda = \textrm{Prob}(i \textrm{ reports heads}|i\textrm{ saw tails}).
\end{equation}

For each simulation and confidence interval, I computed whether the confidence interval contained
the true value of $\lambda$. Table \ref{tab:coverage-just-gsv} shows the results.


```{r params-table}

huxtable(
        Parameter = c(
          "Sample size (N)", 
          "Probability of lying ($\\lambda$)", 
          "Probability of bad random outcome (P)"
        ),
        Values    = c(
          paste(params$N, collapse = ", "),
          paste(params$lambda, collapse = ", "),
          paste(params$P, collapse = ", ")
        ),
        add_colnames = TRUE
      ) %>% 
      set_caption("Parameter values") %>% 
      set_escape_contents(everywhere, 1, FALSE) %>% 
      set_label("tab:params-table") %>% 
      theme_plain()
```



```{r coverage-overall}

coverage <- sim_est %>% 
      group_by(method, CI) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE))

cov_overall <- coverage %>% 
      ungroup() %>% 
      mutate(
        CI = paste0("CI ", CI, "%"),
        Method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian"),
        method = NULL
      ) %>% 
      spread(CI, mean_in_ci) %>% 
      arrange(factor(Method, levels = c("GSV", "Frequentist", "Bayesian"))) 

pct_format <- function (x) sprintf("%.1f%%", x * 100)

ht_overall <- as_huxtable(cov_overall, add_colnames = TRUE) %>% 
      set_caption("Proportion of true results within confidence interval.") %>% 
      set_label("tab:coverage-overall") %>% 
      set_number_format(-1, -1, list(pct_format)) %>% 
      theme_plain()


ht_just_gsv <- ht_overall[1:2,]
label(ht_just_gsv) <- "tab:coverage-just-gsv"
ht_just_gsv[1, 1] <- ""
theme_plain(ht_just_gsv)
```

By definition, 95% of 95% confidence intervals ought to contain the true value, on average. This is
called "achieving nominal coverage". GSV intervals are far from this. The confidence intervals are
too narrow. This problem holds across all simulated probabilities of the low outcome, confidence
levels, and sample sizes.^[See the appendix.]

Why does this happen? We can get a clue by running the GSV method when there are 10 reports of 
"heads" out of 10 for a fair coin flip (*R = N = 10, P = 0.5*). The resulting point estimate is that
100% of subjects lied. The upper and lower 99% confidence intervals are also 100%. NaÃ¯vely, this might
seem wrong. Can we rule out that 8 subjects lied, and the two honest subjects both saw heads?

In fact, the GSV method works like this. First, given *R* reports of heads, the probability that a total 
of *T* "true" heads were observed is calculated as:


\begin{equation}
\label{eqn:truncated-binom}
\textrm{Prob}(T \textrm{ heads}| R; N, P) = \frac{
  \textrm{binom}(T,N, 1 - P)
}{
  \sum^R_{k=0}\textrm{binom}(k,N, 1 - P)
}
\end{equation}


This is the binomial distribution, truncated at *R* because by assumption, nobody "lies downward"
and reports tails when they really saw heads.

Next, from *T* the number of lies told is calculated as $R - T$; and the proportion of lies told
is:


\begin{equation}
\textit{Lies} = \frac{R-T}{N-T} 
\end{equation}


because $N - T$ people saw the low outcome and had the chance to lie. Combining this
with the truncated binomial gives a cumulative distribution function of the proportion of liars.
This is then used to estimate means and confidence intervals.

Putting these together, for $R = N = 10$, the estimated distribution of liars is calculated as follows:

* With probability $\frac{1}{1024}$, there were really 10 heads. Nobody lied in the
sample.^[But the proportion of people who lied *out of those who saw tails* is undefined, because
noone saw tails. The GSV software seems to resolve this by fixing the proportion of lies to 100%.]

* Otherwise, 1 or more people saw tails, and they all lied. The proportion of liars is 100%.

Hence, the lower and upper confidence intervals are all 100%.

There are two problems with this approach. One is statistical, one is conceptual.

```{r}

heads_10 <- sim_est %>% 
      filter(N == 10, R == 10, P == 0.5) %>% 
      select(lambda, heads) 
h10 <- heads_10$heads
l10 <- heads_10$lambda

inv_prob_h <- round(1/mean(h10 == 10), 0)
inv_prob_h_l20 <- round(1/mean(h10[l10 == 0.2] == 10), 0)

```

First, if 10 out of 10 heads are reported, you should learn two things. On the one hand, there are
probably many liars in your sample. On the other hand, probably a lot of coins really landed heads.
The probability distribution in equation (\ref{eqn:truncated-binom}) does not take account of this.
In particular, for *R = N*, it reduces to the binomial distribution. This is not correct. For
example, in my simulations with $P = 0.5$, the overall probability that there were 10 heads when $R
= N = 10$ was about 1 in `r inv_prob_h`, not 1 in 1024. When $\lambda = 0.2$, it is about 1 in 
`r inv_prob_h_l20`. The same is true at the other end of the scale: when *R* is low, we indeed learn
that $T \le R$ but also that the overall distribution is not shaped like the prior.

Second, more importantly, the GSV approach estimates *Lies*, the distribution of the proportion of 
*lies actually told*, among the subsample of people who saw tails. But we are not usually interested
in the proportion of lies actually told. We care about the probability that people in the
sample would lie if they saw tails --  $\lambda$ in equation (\ref{eqn:lambda}). This $\lambda$ can
be interpreted in different ways. It could be that on seeing a tail, each person in the sample lies
with probability $\lambda$. Or it could be that the sample is drawn from a population of whom
$\lambda$ are (always) liars, and $1 - \lambda$ are truth-tellers.

The fact that some people saw heads, and had no reason to lie, is just a nuisance for estimation.
Similarly, we are not interested, per se, in whether more lies were actually told in one group than
another. Suppose that 9 out of 10 people saw heads. Only one person had a reason to lie, so by
definition, *Lies* will be either 0 or 1. This will almost certainly be different from the value of
*Lies* in another group where there were fewer heads. But that tells us very little, either about the 
samples as a whole, or about the population they come from.

*Lies* can be treated as an estimate of $\lambda$. It is unbiased, because the
probability that people would lie if they saw tails is independent of the coin flip they actually
see. But it can be a very noisy estimate. 
Again, suppose 10 heads out of 10 are reported, and 9 heads were really observed. *Lies* is 100%. 
But it is 100% of just one person.

The first of these two problems means that the GSV distribution of *Lies* is wrong. In the appendix
I show that their estimator can have substantial bias. In fact, it performs worse than the naÃ¯ve
estimator from equation (\ref{eqn:naive-estimator}), $\frac{R-(1-P)}{P}$. Also, the GSV confidence
intervals do not always achieve nominal coverage of *Lies*. When the number of heads reported is
either high or low, the percentage of confidence intervals containing *Lies* may fall below the
nominal value.

The second problem means that the GSV confidence intervals are not correct for $\lambda$. We can
tell that the second problem matters, because the confidence interval coverage of $\lambda$ is much
worse than coverage of *Lies*. The GSV estimator is also a biased estimator of $\lambda$.

```{r disagg-coverage}

disagg <- sim_est %>% 
      filter(method=="gsv") %>% 
      group_by(
        `R/N:` = R/N, N, P, CI
      ) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE), prop = n()/nrow(.)) %>% 
      mutate(CI = paste0("CI ", CI, "%")) %>% 
      spread(CI, mean_in_ci) %>% 
      mutate(ok  = `CI 95%` > 0.95)

prop_ok <- disagg %>%  
      ungroup() %>% 
      summarize(prop_ok = sum(prop[ok])) %>% 
      pull(prop_ok)
```

The problem is especially bad when there are many reports of heads. In this case there were probably
many true heads, so *T* is high and the true sample size $N - T$ is low. Table \ref{tab:coverage-rn}
shows this. It splits the simulations by the proportion of reported heads, *R/N*. Coverage levels
fall off sharply as *R/N* increases. Note that for fair coin flips, *R/N* is typically greater than
0.5, in the simulations and in reality.

```{r coverage-rn}
coverage_rn <- sim_est %>% 
      filter(method=="gsv") %>% 
      group_by(
        `R/N:` = factor(Hmisc::cut2(R/N, cuts = c(0, 1/4, 1/2, 3/4, 1)))      
      ) %>% 
      mutate(pct_cases = n()/nrow(.)) %>% 
      group_by(`R/N:`, CI) %>% 
      summarize(
        mean_in_ci  = mean(within_ci, na.rm = TRUE), 
        `Percentage of cases` = pct_cases[1]
      ) %>% 
      mutate(CI = paste0("CI ", CI, "%")) %>% 
      spread(CI, mean_in_ci)
    

as_huxtable(coverage_rn, add_colnames = TRUE) %>% 
      set_caption("GSV confidence interval coverage by proportion of heads reported") %>% 
      set_label("tab:coverage-rn") %>% 
      set_escape_contents(2:3, 1, FALSE) %>% 
      set_number_format(-1, -1, list(pct_format)) %>% 
      theme_plain() %>% 
      set_right_border(-1, 2, 0.4)
```

# Alternative methods

Can we do better? Start with the probability of getting *R* reports of heads in total, given
$\lambda$. Since individuals report heads either if they see heads with probability $1-P$, or if
they see tails but lie, with probability $\lambda P$, this is just:

\begin{equation}
\textrm{Pr}(R|\lambda; N,P) = \textrm{binom}(R, N, (1 - P) + \lambda P)
\end{equation}

That immediately suggests the first method, which is to estimate the parameter of this distribution,
$(1 - P) + \lambda P$, from the proportion of heads reported in the sample, then back out $\lambda$.
This is the conventional method of e.g. @abeler2016preferences. It is justified if the sample is
large, because this will lessen sampling variation in the proportion of actual heads observed.
Similarly, if the sample is large enough, we can generate hypothesis tests for a value of $\lambda$
-- e.g., zero -- by using the tails of the binomial distribution. And we can back out confidence
bounds for $\lambda$ from confidence bounds for the population proportion of heads reported in the
same way. As GSV point out, in small samples, this method runs the risk that the sample proportion
of high outcomes will be different from its expected value.^[In particular, this method can estimate
confidence bounds for $\lambda$ lower than 0. If so, we can set them to 0.]   We'll see whether this
matters.

There are numerous ways to calculate confidence intervals in a test of proportions. See e.g.
@agresti1998approximate. Here, I use the binomial exact test of @clopper1934use, which is 
known to be conservative. I call this the "frequentist" method.

The second method is to use Bayes' rule. Start with a prior probability density function over
the probability of liars, $\phi(\lambda)$. The posterior probability is then:

\begin{equation}
\phi(\lambda | R; N,P) = \frac{
  \textrm{Pr}(R|\lambda; N,P) \phi(\lambda)
}{
  \int_0^1 \textrm{Pr}(R|\lambda'; N,P) \phi(\lambda) \; \textrm{d}\lambda
}
\end{equation}


From this, one can derive confidence intervals and expected values in the usual way.^[R code
to do this is available at https://github.com/hughjonesd/GSV-comment. ] Technically,
they are Bayesian "credible" intervals. I use Highest Posterior Density intervals [@hyndman1996computing], rather than the central confidence interval. This allows the intervals
to include endpoints of the distribution, which is important when e.g. testing for $\lambda = 0$.

The Bayesian method requires a prior. Here, I used a uniform prior, $\phi(\lambda) = 1$ on $[0, 1]$.

I ran these estimation methods on my simulated data. Results are shown in Table
\ref{tab:coverage-allmethods}. Both frequentist and  Bayesian methods mostly achieve the nominal
confidence level, with more than 90/95/99% of intervals containing the true value of *Liars*. The
exception is the frequentist 99% confidence interval, which is too narrow.

```{r coverage-allmethods}
ht_overall %>% 
      set_caption("Coverage levels for GSV and alternative methods") %>% 
      set_label("tab:coverage-allmethods")
```

Frequentist confidence intervals could be less accurate when N is low, since that leads to more
sampling variation in the number of true heads. Table \ref{tab:coverage-N} checks this by looking
separately at simulations with *N = 10* and *N = 50*. Frequentist 99% confidence intervals indeed
appear slightly too narrow for this range. Bayesian confidence intervals are fine. Both alternatives
perform much better than the GSV approach.


```{r coverage-N}

coverage_N <- sim_est %>% 
      filter(N <= 50) %>% 
      group_by(method, CI, N) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        CI = paste0("CI ", CI, "%"),
        Method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian"),
        method = NULL
      ) %>% 
      select(Method, N, CI, mean_in_ci) %>% 
      spread(CI, mean_in_ci) %>% 
      arrange(factor(Method, levels = c("GSV", "Frequentist", "Bayesian"))) 
  

as_huxtable(coverage_N, add_colnames = TRUE) %>% 
      set_rowspan(c(2, 4, 6), "Method", 2) %>% 
      set_valign(c(2, 4, 6), "Method", "top") %>% 
      set_number_format(-1, contains("CI"), list(pct_format)) %>% 
      set_caption("Confidence interval coverage by sample size") %>% 
      set_label("tab:coverage-N") %>% 
      theme_plain() %>% 
      set_background_color(everywhere, 1, NA) %>% 
      set_top_border(c(4, 6), everywhere, 0.4) %>% 
      set_bottom_border(6, 1, 0.4) %>% 
      set_right_border(everywhere, 1, 0.4)
```


# Point estimation


We can also compare the accuracy of point estimates of *Liars*. Table
\ref{tab:point-estimates} shows the mean squared error for methods by different *N*. For low *N*,
the best method is Bayesian and the worst is Frequentist, with GSV in between. When *N* gets large all
methods give about the same estimates and are equally accurate. 

The Bayesian method might have an advantage here, since it assumes a uniform prior and the
simulations indeed used a uniform distribution of the proportion of liars *L/N*. In fact, further
analysis reveals that the Bayesian method is best across all specific values of *L/N* up to
80%.^[When *L/N = 1*, all subjects deterministically report heads, and both Frequentist and
Bayesian point estimates are exactly correct.] So, the Bayesian method is likely to be best unless
one is sure that the true *L/N* is rather high.

```{r point-estimates}

sim_est %>% 
      filter(CI == 90) %>% 
      mutate(
        Method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian"),
        method = NULL
      ) %>% 
      group_by(Method, N) %>% 
      summarize(mean_sq_error = mean(error_sq, na.rm = TRUE)) %>% 
      spread(N, mean_sq_error, sep = ": ") %>% 
      ungroup() %>% 
      as_huxtable(add_colnames = TRUE) %>% 
      set_caption("Mean squared errors by method and $N$") %>% 
      set_label("tab:point-estimates") %>% 
      theme_plain()
```


# Empirical Bayes

Bayesian estimates are accurate, but rely on a choice of prior.
A non-informative prior is a reasonable choice. Alternatively one might use information
from previous meta-analyses such as @abeler2016preferences. If the sample size is large enough,
the choice of prior should not matter much.

When comparing the dishonesty rates of different groups, an interesting approach is to use the
"empirical Bayes" method [@casella1985introduction]. This piece of statistical jiu-jitsu involves
estimating a common prior from the pooled data, before updating the prior for each individual group.


```{r hj-data}

load("honesty.RData")
dfr <- dfr %>% 
      filter(heads %in% 1:2, ! is.na(resnat) , age %in% 2:6)

nats <- dfr %>%  
      group_by(resnat) %>% 
      summarize(
        heads = sum(heads==1), 
        N = n(),
        prop_heads = heads/N,
        prop_liars = 2*prop_heads - 1
      )


pp <- MASS::fitdistr(nats$prop_liars, "beta", start = list(
        shape1 = 1, shape2 = 1
      ))
pooled_prior <- function (x) dbeta(x, pp$estimate["shape1"], pp$estimate["shape2"])

distribs <- apply(nats[, c("heads", "N")], 1, function (r) {
  N <- r["N"]
  update_prior(
      heads = r["heads"],
      N = N,
      P = 0.5, 
      prior = pooled_prior
  )
})

names(distribs) <- nats$resnat

nat_means <- sapply(distribs, dist_mean)
nat_cis <- sapply(distribs, dist_hdr, conf_level = 0.95)
rownames(nat_cis) <- c("ci.low", "ci.high")

nats <- cbind(nats, t(nat_cis), mean = nat_means)

nats %<>% arrange(mean)

# nats$resnat <- factor(nats$resnat, levels = nats$resnat)
```

For example, @hugh2016honesty estimates the dishonesty rates of 15 nations using a coin flip
experiment. We can fit a beta distribution using the 15 observations of *2R/N - 1*. This will be
our prior.

```{r hj2016-pic, fig.cap = "Posterior means and confidence intervals for proportion dishonest in 15 countries. Crosses are naÃ¯ve estimates.", fig.align="center", fig.width=5.5, fig.height=4}

ggplot(nats, aes(x = factor(resnat, levels = resnat), y = mean, fill = mean)) +
      geom_col(alpha = 0.8) +
      geom_linerange(aes(ymin = ci.low,ymax = ci.high)) +
      geom_point(aes(y = prop_liars), shape = 4, size = 3, colour = "red") +
      xlab("Nation") + ylab("Prop. liars") +
      scale_fill_viridis_c(option = "A", guide = "none") + 
      theme_light()+
      coord_flip()

```


Results are shown in Figure \ref{fig:hj2016-pic}, with means and 95% confidence intervals. There is
some "shrinkage" towards the pooled mean from the naÃ¯ve per-country estimates found by calculating
*2R/N - 1* separately for each country). One of the strengths of empirical Bayes,  as
@casella1985introduction points out, is that it "anticipates regression to the mean". That is, as
there is noise in the data, the most and least honest groups in the data probably had some extreme
error terms. So the British people in the sample may be less honest, and the Chinese people more
honest, than the per-group data suggest.

# Bayesian hypothesis testing

We can also test hypotheses using the Bayesian approach. If two samples are independent, then the 
probability that e.g. the true proportion of liars in sample 1 is smaller than in sample 2 can be
calculated from the marginal distributions in each sample:

\[
\int^1_0 \int_0^{\lambda_1} \phi(\lambda_1)\phi(\lambda_2) \; \mathrm{d}\lambda_2 \mathrm{d}\lambda_1
\]


Table \ref{tab:country-comparisons} shows Bayesian comparisons for selected countries. Even though
estimates have "shrunk" towards the pooled mean from the naive estimates, differences remain
significant, partly because the pooled prior has made posterior confidence intervals narrower.
Similarly, we can calculate the full distribution of the differences between countries.


```{r country-comparisons}

prob_country_more <- function (ctry1, ctry2) {
  compare_dists(distribs[[ctry1]], distribs[[ctry2]])
}

ctry_comp <- outer(nats$resnat, nats$resnat, Vectorize(prob_country_more))
diag(ctry_comp) <- NA
rownames(ctry_comp) <- nats$resnat
colnames(ctry_comp) <- nats$resnat

selected <- c("GB", "ZA", "GR", "BR", "CN")
ctry_comp <- ctry_comp[selected, selected]
cc_hux <- as_huxtable(ctry_comp, add_colnames = TRUE, add_rownames = TRUE) %>% 
      set_caption("Bayesian country comparisons. 
            Cells give the probability that the row country has higher proportion of liars.") %>% 
      set_label("tab:country-comparisons") %>% 
      set_na_string("--") %>% 
      set_number_format("%.3f") %>% 
      set_align(-1, -1, ".") %>% 
      theme_plain() %>% 
      set_bold(everywhere, 1, TRUE)

cc_hux[1, 1] <- ''

cc_hux

```


```{r GB-vs-RU-pic, fig.cap = "Probability distribution of difference in proportion of liars, GB vs RU", fig.width=4, fig.height=3, fig.align="left", eval = FALSE}

diff_GB_RU <- difference_dist(distribs$GB, distribs$RU)
curve(diff_GB_RU, xlim = c(-1, 1), ylab = "Density", xlab = expression(lambda[GB] - lambda[RU]))
```

# Conclusion

These results suggest some recommendations when you are analysing a coin-flip
style experiment.

1. Do not use the GSV confidence intervals, unless you are specifically interested in *Lies* rather
  than $\lambda$. Even in that case, be aware that the confidence intervals have low coverage for 
  extreme values of *R*.
  
2. Do not use the GSV estimates. They are biased as estimators of both *Lies* and $\lambda$.
  
3. If your N is reasonably large, say at least 100, you can safely use standard frequentist 
  confidence intervals and tests.
  
4. If your N is small, consider Bayesian estimates and confidence intervals. To estimate differences 
  between subgroups, consider empirical Bayes with a prior derived from the pooled sample.

Lastly, I suggest a simple way to avoid these statistical shenanigans. Ask a group of *N*
experimental subjects, not to flip a coin, but to draw a black or white ball, without replacement,
from an urn containing *N* balls, of which *T* are white. Pay them if they report a white ball. You
will then know -- not estimate -- that the proportion of lies told is $(R - T)/(N - T)$, and this
will also be the proportion of liars, in a sample of exactly $N - T$ subjects. So you can
simultaneously estimate the proportion of lies, and of the lying liars who tell them.

\FloatBarrier
\newpage

# Appendix


```{r methods-scale}

scale_colour_methods <- scale_color_brewer(type = "qual", palette = 2)
```

R code to reproduce this comment is available at https://github.com/hughjonesd/GSV-comment, along with
code to find Bayesian posterior distributions of $\lambda$.

The errors in GSV confidence intervals could be due to a programming error rather than to the
algorithm. I could reproduce GSV's expected value to 4 or 5 significant figures by following their
method, but I could not reproduce their confidence intervals. For example, 
when $R = 3, N = 6, P = 0.5$, GSV's Java program gives the upper bound of the 95% confidence 
interval for the proportion of liars as 49.91%. But the possible proportions of lies when there
are $T = 0, 1, 2, 3$ true heads, are $(R-T)/(N-T) =$
`r paste0(prop_liars(3:0, 3, 6) * 100, "%", collapse = ", ")`.
It may be that some linear interpolation is being done. 

Nevertheless, if I use GSV's method as stated, rather than their program, confidence intervals
remain too small, as shown in Table \ref{tab:coverage-recalc}.

```{r coverage-recalc}

coverage_recalc <- sim_est_recalc %>% 
      filter(method == "recalc") %>% 
      group_by(CI) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        CI = paste0("CI: ", CI, "%"),
      ) %>% 
      spread(CI, mean_in_ci) 

as_huxtable(coverage_recalc, add_colnames = TRUE) %>% 
      set_caption("Proportion of true results within confidence interval, recalculated GSV method") %>% 
      set_label("tab:coverage-recalc") %>% 
      set_number_format(-1, everywhere, list(pct_format)) %>% 
      theme_plain()

```

Figure \ref{fig:coverage-grp-pic} shows the proportions of true values within the confidence interval
for the GSV method, split by *N*, *P*, confidence level and $\lambda$. Dashed lines show the nominal
confidence level. This makes the pattern clear: coverage gets worse as $\lambda$ increases. (At
100%, coverage jumps back up since results become deterministic). Also, coverage does not get
better as *N* increases.


```{r coverage-grp-pic, fig.cap = "GSV confidence interval coverage by confidence level, N, P and $\\lambda$", fig.align="left", fig.width=6, fig.height=4}

pic_method <- "gsv"
# pic_method <- "ubayes"

coverage_grp_gsv <- sim_est %>% 
      filter(method == pic_method) %>% 
      group_by(CI, N, lambda, P) %>% 
      summarize(mean_in_ci = mean(within_ci, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        P = factor(P),
        ci = CI,
        CI = paste0(CI, "%")
      )

ggplot(coverage_grp_gsv, aes(lambda, mean_in_ci, colour = P)) +
      geom_point() + geom_line() +
      geom_hline(aes(yintercept = ci/100), linetype = 2) +
      facet_grid(N ~ CI, labeller = "label_both") +
      xlab(expression(lambda)) + ylab("Coverage") 

```

Figure \ref{fig:bias-by-lambda-pic} shows the average bias of expected values by different methods.
At N = 500, all methods perform reasonably well. For lower values, there is a clear pattern: 
Bayesian methods are least biased, GSV method is most biased, and the frequentist method is
in between.

```{r bias-by-lambda-pic, fig.cap = "Bias by method and $\\lambda$", fig.align="left", fig.width=5.5, fig.height=4}

bias_lambda <- sim_est %>%       
      filter(CI == 90) %>% 
      group_by(method, N, P) %>% 
      summarize(Bias = mean(error, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian")
      ) %>% 
      rename(Method = method)


ggplot(bias_lambda, aes(Method, Bias, colour = Method)) + 
      geom_point() + scale_x_discrete(breaks = NULL) +
      facet_grid(N ~ P, labeller = "label_both", scales = "free") +
      xlab("") +
      scale_colour_methods

```

Figure \ref{fig:errors-by-true-prop} shows mean squared error by estimation method and $\lambda$,
for *N* of 10 and 50. The Bayesian method is best for all values of *Liars* up to 80%.

```{r errors-by-true-prop, fig.cap = "Errors by method and $\\lambda$, N = 10 and 50", fig.align="left", fig.width=5.5, fig.height=4}

errors_lambda <- sim_est %>%       
      filter(CI == 90, N <= 50) %>% 
      group_by(method, lambda) %>% 
      summarize(mean_sq_error = mean(error_sq, na.rm = TRUE)) %>% 
      ungroup() %>% 
      mutate(
        method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian")
      ) %>% 
      rename(Method = method)

ggplot(errors_lambda, aes(lambda, mean_sq_error, colour = Method)) + 
      scale_x_continuous(breaks = 0:5/5) + 
      geom_line() + geom_point() +
      scale_colour_methods + 
      xlab(expression(lambda)) + ylab("Mean square error")

```

```{r errors-disagg, include = FALSE, fig.align="left", fig.width=5.5, fig.height=4, fig.cap="Point estimate errors by R/N, N and P"}

errors_disagg <- sim_est %>%       
      filter(CI == 90, N <= 50) %>% 
      group_by(rn = findInterval(R/N, 0:10/10, rightmost.closed = TRUE), N, P, method) %>% 
      summarize(mean_sq_error = mean(error_sq, na.rm = TRUE)) %>% 
      mutate(
        method = recode(method, gsv = "GSV", simple = "Frequentist", ubayes = "Bayesian")
      ) %>% 
      rename(`R/N` = rn, Method = method)

g <- ggplot(errors_disagg, aes(`R/N`, mean_sq_error, colour = Method)) + 
      geom_line() + geom_point() +
      facet_wrap(~N + P, scales = "free_y") + 
      scale_colour_methods +
      ylab("Mean square error") 
      
```

 

```{r gsv-lies-coverage, cache = cache_results}

# if we assume that liars are not uniformly distributed, the posterior of number of heads
# is not like the prior
nreps_l <- 2000L

check_coverage_lies <- function(N, CL, lambda, P) {

  L <- rbinom(nreps_l, N, lambda) # how many people will lie if they see tails?
  HL <- rbinom(nreps_l, L, 1 - P)
  HNL <- rbinom(nreps_l, N - L, 1 - P)
  R <- L + HNL
  TH <- HL + HNL
  sim_inputs <- data.frame(L, R, TH, N, CL, P, lambda, row.names = NULL)

  sim_in_uq <- distinct(sim_inputs[, c("CL", "R", "N", "P")])
  sim_results <- apply(sim_in_uq, 1, function (r) {
    with(as.list(r), 
      c(prop_liars_ev_gsv(R, N, P), prop_liars_ci_gsv(CL, R, N, P))
    )
  })
  sims_in_uq <- cbind(sim_in_uq, t(sim_results))
  
  sims <- left_join(sim_inputs, sims_in_uq, by = names(sim_in_uq))
  names(sims) <- c('L', 'R', 'TH', 'N', 'CL', 'P', 'lambda', 'ev', 'cilo', 'cihi')
  
  return(sims)
}

# reuse our parameters
lies_params <- tidyr::crossing(
        N      = params$N,
        lambda = params$lambda,
        P      = params$P,
        CL     = ci_levels/100
      )

lies_sims <- apply(lies_params, 1, function (x) check_coverage_lies(
      x["N"], x["CL"], x["lambda"], x["P"]))
lies_sims <- bind_rows(lies_sims)
lies_sims <- right_join(lies_params, lies_sims, by = names(lies_params))

lies_sims %<>% 
      mutate(
        cl          = CL,
        CL          = paste("CI: ", CL * 100, "%"),
        ci_width    = cihi - cilo,
        lies_told   = (R - TH)/(N - TH),
        lies_in_ci  = cilo <= lies_told & lies_told <= cihi,
        error       = ev - lies_told,
        naive_ev    = pmax(0, (R/N - (1 - P))/P),
        naive_error = naive_ev - lies_told,
        P = factor(P)
      ) %>% 
      group_by(P, N, lambda) %>% 
      mutate(
        # within-group quantiles
        RN = ntile(R/N, 10)
      ) %>% 
      ungroup()


# confints ought to look like real dist, over R, N and P.
qs <- c(0.005, 0.025, 0.05, 0.95, 0.975, 0.995)
lies_sims %<>% group_by(R, N, P) %>%
      do(lies_quantiles = quantile(.$lies_told, qs, na.rm = TRUE)) %>% 
      unnest(lies_quantiles) %>% 
      mutate(quantile = paste0("q", rep_len(qs, nrow(.)))) %>% 
      spread(quantile, lies_quantiles) %>% 
      inner_join(lies_sims, by = c("R", "N", "P")) %>% 
      mutate(
        true_cihi = case_when(
          cl == 0.99 ~ q0.995,
          cl == 0.95 ~ q0.975,
          cl == 0.90 ~ q0.95
        ),
        true_cilo = case_when(
          cl == 0.99 ~ q0.005,
          cl == 0.95 ~ q0.025,
          cl == 0.90 ~ q0.05
        )
      ) %>% 
      select(-starts_with("q0"))
  
```

## GSV as an estimate of *Lies*

GSV argue that their method provides a good estimate of *Lies*, as opposed to $\lambda$. Here
I check whether that is true.

I ran `r nreps_l` simulations for each of the parameter combinations. I calculated *Lies* as
*(R - T)/(N - T)*, and ignored simulations where *N = T*. I estimated confidence intervals and
expected value using the GSV method. For a comparison I also estimated the expected value of 
*Lies* using the "naÃ¯ve" estimator $\max(0, \frac{R - (1 - P))}{P})$.

Figure \ref{fig:gsv-lies-bias-pic} shows average bias by sample size, *P* and true *Lies*. For low
sample sizes, both methods perform about the same. For higher sample sizes and low values of *Lies*, however, the GSV method is clearly dominated by the naÃ¯ve method, and shows a lot of upward bias
-- more than 5 percentage points even when *N = 100*. This is especially problematic for testing 
whether anyone lied in the sample.


```{r gsv-lies-bias-pic, fig.cap = "Bias of GSV method for 'Lies'.", fig.align="center", fig.width=5.5, fig.height=4, warning=FALSE}

lies_sims$lt_cut <- cut(lies_sims$lies_told, 0:10/10, labels = FALSE)

my_alph <- 0.8
g <- ggplot(lies_sims, aes(lt_cut, error, color = "GSV", linetype = "GSV")) + 
      stat_summary(fun.y = mean, geom = "point", alpha = my_alph) + 
      stat_summary(group = 1, fun.y = mean, geom = "line", alpha = my_alph) + 
      stat_summary(aes(y = naive_error, color = "Naive"), fun.y = mean, geom = "point", 
            alpha = my_alph) + 
      stat_summary(aes(y = naive_error, linetype = "Naive", color = "Naive"), group=1, fun.y = mean, 
            geom = "line", alpha = my_alph) + 
      facet_grid(N ~ P, labeller = "label_both", scales = "free_y")
g +
      geom_hline(yintercept = 0) +
      scale_x_continuous(breaks = seq(0, 10, 2), labels = seq(0, 1, 0.2)) +
      scale_linetype_discrete(name = "Method", guide = guide_legend(title = "Method")) +
      scale_color_manual(name = "Method", values = c("darkred", "blue")) +
      xlab("Lies") + ylab("Bias") 
```


Figure \ref{fig:gsv-lies-coverage-pic} shows the proportion of confidence intervals that contain the true
value of *Lies*. Coverage is shown by confidence level, *N*, *P* and the decile (within these groups) of
proportion of heads reported (*R/N*). Overall results are quite solid, but when the proportion of
heads reported is low or high, coverage drops below the nominal intervals. Interestingly, this problem
gets worse as $N$ increases.

 

```{r gsv-lies-coverage-pic, fig.cap = "Coverage of GSV method for 'Lies'.", fig.align="center", fig.width=5.5, fig.height=4}


# you can do this to find where confints are wrong. If the red line is above the diagonal,
# cilo is too high; if the blue line is below it, cihi is too low.
# ggplot(lies_sims) + 
#       geom_smooth(aes(true_cihi, cihi), color = "blue") + 
#       geom_smooth(aes(true_cilo, cilo), color = "red") + 
#       geom_abline(slope = 1, intercept = 0) + 
#       facet_grid(N ~ CL) + theme_minimal() + xlab("True bound") + 
#       labs(title = "Red line: low ci; blue line: high ci")

# lsims_summary <- lies_sims %>% 
#       group_by(lambda, P, N, CI, ci) %>% 
#       summarize(
#         Coverage = mean(lies_in_ci, na.rm = TRUE)
#       )

# ggplot(lsims_summary, aes(lambda, Coverage, colour = P)) + 
#       geom_point() + geom_line() + 
#       geom_hline(aes(yintercept = ci), linetype = 2) +
#       facet_grid(N ~ CI) + xlab(expression(lambda))

lsims_by_R <- lies_sims %>% 
      group_by(P, N, CL, RN, cl) %>%   # having cl keeps it in the data
      summarize(
        Coverage = mean(lies_in_ci, na.rm = TRUE)
      )

ggplot(lsims_by_R, aes(RN, Coverage, colour = P)) + 
      geom_point() + geom_line() + 
      geom_hline(aes(yintercept = cl), linetype = 2) +
      facet_grid(CL ~ N, scales = "free", labeller = "label_both") +
      xlab("R/N (decile)") + 
      scale_x_continuous(breaks = 0:5 * 2)


```



\FloatBarrier

\newpage

# References
